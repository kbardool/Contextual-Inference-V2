{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Pipeline -  (build model and load weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T16:45:48.766805Z",
     "start_time": "2018-12-24T16:45:48.759799Z"
    }
   },
   "outputs": [],
   "source": [
    "# np_format = {}\n",
    "# float_formatter = lambda x: \"%10.4f\" % x\n",
    "# int_formatter   = lambda x: \"%10d\" % x\n",
    "# np_format['float'] = float_formatter\n",
    "# np_format['int']   = int_formatter\n",
    "# np.set_printoptions(linewidth=195, precision=4, floatmode='fixed', threshold =10000, formatter = np_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:37:40.454241Z",
     "start_time": "2019-02-03T14:37:28.436680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working dir:  E:\\git_projs\\MRCNN3\\notebooks\n",
      "appending '..' to sys.path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "# import matplotlib.pyplot as plt\n",
    "# from   mpl_toolkits.mplot3d import Axes3D\n",
    "# from importlib import reload\n",
    "# reload(plt)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys,os, pprint, pickle\n",
    "pp = pprint.PrettyPrinter(indent=2, width=100)\n",
    "print('Current working dir: ', os.getcwd())\n",
    "if '..' not in sys.path:\n",
    "    print(\"appending '..' to sys.path\")\n",
    "    sys.path.append('..')\n",
    "    \n",
    "import numpy as np\n",
    "import mrcnn.utils     as utils\n",
    "import mrcnn.visualize as visualize\n",
    "import mrcnn.prep_notebook as prep\n",
    "from mrcnn.prep_notebook import build_fcn_evaluate_pipeline, run_mrcnn_evaluate_pipeline, get_evaluate_batch\n",
    "from mrcnn.visualize     import display_training_batch\n",
    "from mrcnn.newshapes     import prep_newshape_dataset\n",
    "from mrcnn.utils         import trim_zeros, compute_overlaps\n",
    "from mrcnn.calculate_map import update_map_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T14:49:30.957614Z",
     "start_time": "2019-01-24T14:49:30.613370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Arguments passed :\n",
      "   --------------------\n",
      "   batch_size                     1\n",
      "   coco_classes                   None\n",
      "   epochs                         1\n",
      "   evaluate_method                2\n",
      "   fcn_arch                       FCN8L2\n",
      "   fcn_layers                     ['all']\n",
      "   fcn_logs_dir                   train_fcn8_l2_newshapes\n",
      "   fcn_losses                     fcn_BCE_loss\n",
      "   fcn_model                      last\n",
      "   last_epoch                     0\n",
      "   lr                             0.001\n",
      "   mrcnn_exclude_layers           None\n",
      "   mrcnn_layers                   ['mrcnn', 'fpn', 'rpn']\n",
      "   mrcnn_logs_dir                 train_mrcnn_newshapes\n",
      "   mrcnn_model                    last\n",
      "   new_log_folder                 False\n",
      "   opt                            ADAGRAD\n",
      "   scale_factor                   1\n",
      "   steps_in_epoch                 1\n",
      "   sysout                         SCREEN\n",
      "   val_steps                      1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##------------------------------------------------------------------------------------\n",
    "## Parse command line arguments\n",
    "##------------------------------------------------------------------------------------\n",
    "parser = utils.command_line_parser()\n",
    "input_parms  =\" --batch_size     1  \"\n",
    "input_parms +=\" --mrcnn_logs_dir train_mrcnn_newshapes \"\n",
    "# input_parms +=\" --fcn_logs_dir   train_fcn8_bce \" \n",
    "input_parms +=\" --fcn_logs_dir   train_fcn8_l2_newshapes \"\n",
    "input_parms +=\" --mrcnn_model    last \"\n",
    "input_parms +=\" --fcn_model      last \"\n",
    "input_parms +=\" --fcn_layer      all\"\n",
    "input_parms +=\" --fcn_arch       fcn8L2 \" \n",
    "input_parms +=\" --sysout         screen \"\n",
    "\n",
    "eval_method = '2'\n",
    "input_parms +=\" --evaluate_method \"+eval_method\n",
    "\n",
    "input_parms +=\" --scale_factor    1\"\n",
    "\n",
    "args = parser.parse_args(input_parms.split())\n",
    "utils.display_input_parms(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T14:59:01.130773Z",
     "start_time": "2019-01-24T14:58:41.976172Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODE IS: evaluate\n",
      "\n",
      "--> Execution started at: 01-24-2019 @ 15:58:42\n",
      "    Tensorflow Version: 1.8.0   Keras Version : 2.2.0 \n",
      "    Build_mrcnn_inference_pipeline_newshapes MODE is : evaluate\n",
      "\n",
      "   Arguments passed :\n",
      "   --------------------\n",
      "   batch_size                     1\n",
      "   coco_classes                   None\n",
      "   epochs                         1\n",
      "   evaluate_method                2\n",
      "   fcn_arch                       FCN8L2\n",
      "   fcn_layers                     ['all']\n",
      "   fcn_logs_dir                   train_fcn8_l2_newshapes\n",
      "   fcn_losses                     fcn_BCE_loss\n",
      "   fcn_model                      last\n",
      "   last_epoch                     0\n",
      "   lr                             0.001\n",
      "   mrcnn_exclude_layers           None\n",
      "   mrcnn_layers                   ['mrcnn', 'fpn', 'rpn']\n",
      "   mrcnn_logs_dir                 train_mrcnn_newshapes\n",
      "   mrcnn_model                    last\n",
      "   new_log_folder                 False\n",
      "   opt                            ADAGRAD\n",
      "   scale_factor                   1\n",
      "   steps_in_epoch                 1\n",
      "   sysout                         SCREEN\n",
      "   val_steps                      1\n",
      "\n",
      "\n",
      ">>> Initialize Paths\n",
      " windows  Windows\n",
      ">>> Initialize ModelBase model \n",
      "   Mode      :  evaluate\n",
      "   Model dir :  F:\\models_newshapes\\train_mrcnn_newshapes\n",
      ">>> ModelBase initialiation complete\n",
      ">>> ---Initialize MRCNN model, mode:  evaluate\n",
      " load class+predcition_info from : F:\\PretrainedModels\\newshapes_predicted_classes_info.pkl\n",
      "\n",
      "----------------------------\n",
      ">>> Resnet Graph \n",
      "----------------------------\n",
      "     Input_image shape : (?, 128, 128, 3)\n",
      "\n",
      ">>> Feature Pyramid Network (FPN) Graph \n",
      "\n",
      ">>> RPN Layer \n",
      "\n",
      ">>> RPN Outputs  <class 'list'>\n",
      "      rpn_class_logits/rpn_class_logits:0\n",
      "      rpn_class/rpn_class:0\n",
      "      rpn_bbox/rpn_bbox:0\n",
      "\n",
      ">>> Proposal Layer - generate  1000  proposals\n",
      "    Init complete. Size of anchors:  (4092, 4)\n",
      "\n",
      ">>> FPN Classifier Graph \n",
      "\n",
      ">>> Detection Layer (Evaluation Mode :2)\n",
      "    Detection Layer : call()  <class 'list'> 2\n",
      "\n",
      "--------------------------------\n",
      ">>>  CHM Inference Layer  \n",
      "--------------------------------\n",
      "  > CHM Inference Layer: call  <class 'list'> 1\n",
      "     detections.shape     : (None, 64, 7)\n",
      "\n",
      "  > build_predictions Inference mode ()\n",
      "    config image shape     :  [128 128   3] h: 128 w: 128\n",
      "    Detection Max Instacnes:  64\n",
      "    num_rois               :  64\n",
      "    num_cols               :  7\n",
      "    detected_rois.shape    :  (None, 64, 7)\n",
      "    batch_grid:  (1, 64)\n",
      "    bbox_grid :  (1, 64)\n",
      "    shape of sequence      :  <unknown>\n",
      "    pred_array             :  <unknown>\n",
      "    scatter_ind            :  (1, 64, 3)\n",
      "    pred_scatter           :  (1, 7, 64, 8)\n",
      "    - Add normalized score --\n",
      "\n",
      "    normalizer             :  (1, 7, 1)\n",
      "    norm_score             :  (1, 7, 64, 1)\n",
      "    pred_scatter           :  (1, 7, 64, 9)\n",
      "    sort_inds              :  (1, 7, 64) Keras Tensor: False\n",
      "    class_grid             :  (1, 7, 64) Keras Tensor: False\n",
      "    batch_grid             :  (1, 7, 64) Keras Tensor: False\n",
      "    roi_grid shape         :  (1, 7, 64) Keras Tensor: False\n",
      "    gather_inds            :  (1, 7, 64, 3) Keras Tensor: False\n",
      "    pred_tensor            :  (1, 7, 64, 9) Keras Tensor: False\n",
      "\n",
      " \n",
      "  > build_inference_heatmap() for  ['pred_heatmap']\n",
      "    in_tensor shape        :  (1, 7, 64, 9)\n",
      "    num bboxes per class   :  64\n",
      "    heatmap scale        :  1 Dimensions:  w: 128  h: 128\n",
      "    pt2_sum shape  :  (1, 7, 64)\n",
      "    pt2_ind shape  :  (?, 3)\n",
      "    pt2_dense shape:  (?, 9)\n",
      "    X/Y shapes : (128, 128) (128, 128)\n",
      "    Ones:     (?, 1, 1)\n",
      "    ones_exp * X (?, 1, 1) * (128, 128) =  (?, 128, 128)\n",
      "    ones_exp * Y (?, 1, 1) * (128, 128) =  (?, 128, 128)\n",
      "    pos_grid before transpse :  (?, 128, 128, 2)\n",
      "    pos_grid after transpose :  (128, 128, ?, 2)\n",
      "    >> input to MVN.PROB: pos_grid (meshgrid) shape:  (128, 128, ?, 2)\n",
      "     Prob_grid shape from mvn.probe:  (128, 128, ?)\n",
      "     Prob_grid shape after tanspose:  (?, 128, 128)\n",
      "    << output probabilities shape  :  (?, 128, 128)\n",
      "    old_style_scores        : (1, 7, 64, 3) (1, 7, 64, 3)\n",
      "    prob_grid_clipped :  (?, 128, 128)\n",
      "\n",
      "    normalization ------------------------------------------------------\n",
      "    normalizer     :  (?, 1, 1)\n",
      "    prob_grid_cns: clipped/normed/scaled :  (?, 128, 128)\n",
      "    alt_scores_1    :  (None, 3)  Keras tensor  False\n",
      "    alt_scores_1(by class)       :  (1, 7, 64, 3)  Keras tensor  False\n",
      "    alt_scores_1_norm(by_class)  :  (1, 7, 64, 3) (1, 7, 64, 3)\n",
      "\n",
      "    Scatter out the probability distributions based on class --------------\n",
      "    pt2_ind shape      :  (?, 3)\n",
      "    prob_grid_clippped :  (?, 128, 128)\n",
      "    gauss_heatmap      :  (1, 7, 64, 128, 128)\n",
      "\n",
      "    Reduce SUM based on class and normalize within each class -------------------------------------\n",
      "    gaussian_heatmap_sum :  (1, 7, 128, 128) Keras tensor  False\n",
      "    normalizer shape   :  (1, 7, 1, 1)\n",
      "    normalized heatmap :  (1, 7, 128, 128)  Keras tensor  False\n",
      "    hm_indices shape         : (?, 2) (None, 2)\n",
      "    pt2_heatmaps             : (?, 128, 128) (None, 128, 128)\n",
      "    alt_scores_2    :  (None, 3)  Keras tensor  False\n",
      "    alt_scores_2(scattered)       :  (1, 7, 64, 3)  Keras tensor  False\n",
      "    alt_scores_2_norm(by_class)  :  (1, 7, 64, 3) (1, 7, 64, 3)\n",
      "    reshaped heatmap   :  (1, 128, 128, 7)  Keras tensor  False\n",
      "    gauss_scores    :  (1, 7, 64, 24)  Keras tensor  False\n",
      "    complete\n",
      "\n",
      "    Output of CHMLayerInference: \n",
      "     pred_tensor        :  (1, 7, 64, 9) Keras tensor  False\n",
      "     pred_heatmap_norm  :  (1, 128, 128, 7) Keras tensor  False\n",
      "     pred_heatmap_scores:  (1, 7, 64, 24) Keras tensor  False\n",
      "     complete\n",
      "    shape of pr_hm        :        :  shape: (1, 128, 128, 7)      KB.shape:(None, 128, 128, 7)   Keras Tensor: True\n",
      "    shape of pr_hm_scores :        :  shape: (1, 7, 64, 24)        KB.shape:(None, 7, 64, 24)     Keras Tensor: True\n",
      "\n",
      ">>> Build MaskRCNN build complete. mode:  evaluate\n",
      ">>> MaskRCNN initialiation complete. Mode:  evaluate\n",
      "\n",
      " MRCNN Configuration Parameters \n",
      " ------------------------------ \n",
      "\n",
      "Configuration Parameters:\n",
      "-------------------------\n",
      "BACKBONE_SHAPES                [[32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]\n",
      " [ 2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        64\n",
      "DETECTION_MIN_CONFIDENCE       0.1\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "DETECTION_PER_CLASS            64\n",
      "DIR_DATASET                    F:\\MLDatasets\n",
      "DIR_PRETRAINED                 F:\\PretrainedModels\n",
      "DIR_TRAINING                   F:\\models_newshapes\n",
      "EPOCHS_TO_RUN                  0\n",
      "EVALUATE_METHOD                2\n",
      "FCN_INPUT_SHAPE                [128 128]\n",
      "GPU_COUNT                      1\n",
      "HEATMAP_SCALE_FACTOR           1\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_BUFFER                   20\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LAST_EPOCH_RAN                 0\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MAX_SHAPES_PER_IMAGE           15\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "MIN_SHAPES_PER_IMAGE           1\n",
      "NAME                           mrcnn\n",
      "NEW_LOG_FOLDER                 False\n",
      "NUM_CLASSES                    7\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRED_CLASS_INFO_PATH           F:\\PretrainedModels\\newshapes_predicted_classes_info.pkl\n",
      "RESNET_MODEL_PATH              F:\\PretrainedModels\\resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "ROI_GT_IOU_THRESHOLD           0.2\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "ROI_PROPOSAL_AREA_THRESHOLD    0\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "SHAPES_MODEL_PATH              F:\\PretrainedModels\\mask_rcnn_shapes.h5\n",
      "STEPS_PER_EPOCH                100\n",
      "SYSOUT                         SCREEN\n",
      "TRAINING_PATH                  F:\\models_newshapes\\train_mrcnn_newshapes\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "VERBOSE                        0\n",
      "VGG16_MODEL_PATH               F:\\PretrainedModels\\vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n",
      "\n",
      " MRCNN IO Layers \n",
      " --------------- \n",
      " Inputs:\n",
      " -------\n",
      " index:  0    input name : input_image:0                              Type: float32           Shape: (?, 128, 128, 3)\n",
      " index:  1    input name : input_image_meta:0                         Type: float32           Shape: (?, ?)\n",
      " index:  2    input name : input_gt_class_ids:0                       Type: int32             Shape: (?, ?)\n",
      " index:  3    input name : input_gt_boxes:0                           Type: float32           Shape: (?, ?, 4)\n",
      " Outputs:\n",
      " --------\n",
      " layer:  0    output name: mrcnn_detection_evaluate/detections:0      Type: float32           Shape: <unknown>\n",
      " layer:  1    output name: ROI/rpn_roi_proposals:0                    Type: float32           Shape: (1, ?, ?)\n",
      " layer:  2    output name: mrcnn_class_lambda/mrcnn_class:0           Type: float32           Shape: (?, 1000, 7)\n",
      " layer:  3    output name: mrcnn_bbox_lambda/mrcnn_bbox:0             Type: float32           Shape: (?, 1000, 7, 4)\n",
      " layer:  4    output name: cntxt_layer/pred_heatmap:0                 Type: float32           Shape: (1, 128, 128, 7)\n",
      " layer:  5    output name: cntxt_layer/pred_heatmap_scores:0          Type: float32           Shape: (1, 7, 64, 24)\n",
      "-----------------------------------------------\n",
      " Load Model with init parm: [ last ]\n",
      " Exclude layers: \n",
      "-----------------------------------------------\n",
      " ---> last\n",
      "   Last file is : ('F:\\\\models_newshapes\\\\train_mrcnn_newshapes\\\\mrcnn20181216T0000', 'F:\\\\models_newshapes\\\\train_mrcnn_newshapes\\\\mrcnn20181216T0000\\\\mrcnn_0472.h5')\n",
      ">>> load_weights() from : F:\\models_newshapes\\train_mrcnn_newshapes\\mrcnn20181216T0000\\mrcnn_0472.h5\n",
      "    Weights file loaded: F:\\models_newshapes\\train_mrcnn_newshapes\\mrcnn20181216T0000\\mrcnn_0472.h5 \n",
      "==========================================\n",
      "MRCNN  MODEL Load weight file COMPLETE \n",
      "==========================================\n",
      ">>> Initialize Paths\n",
      " windows  Windows\n",
      "\n",
      "   Arguments passed :\n",
      "   --------------------\n",
      "   batch_size                     1\n",
      "   coco_classes                   None\n",
      "   epochs                         1\n",
      "   evaluate_method                2\n",
      "   fcn_arch                       FCN8L2\n",
      "   fcn_layers                     ['all']\n",
      "   fcn_logs_dir                   train_fcn8_l2_newshapes\n",
      "   fcn_losses                     fcn_BCE_loss\n",
      "   fcn_model                      last\n",
      "   last_epoch                     0\n",
      "   lr                             0.001\n",
      "   mrcnn_exclude_layers           None\n",
      "   mrcnn_layers                   ['mrcnn', 'fpn', 'rpn']\n",
      "   mrcnn_logs_dir                 train_mrcnn_newshapes\n",
      "   mrcnn_model                    last\n",
      "   new_log_folder                 False\n",
      "   opt                            ADAGRAD\n",
      "   scale_factor                   1\n",
      "   steps_in_epoch                 1\n",
      "   sysout                         SCREEN\n",
      "   val_steps                      1\n",
      "\n",
      "\n",
      "\n",
      "   Paths:\n",
      "   -------------------------\n",
      "COCO_DATASET_PATH              F:\\MLDatasets\\coco2014\n",
      "COCO_HEATMAP_PATH              F:\\MLDatasets\\coco2014_heatmaps\n",
      "COCO_MODEL_PATH                F:\\PretrainedModels\\mask_rcnn_coco.h5\n",
      "DIR_DATASET                    F:\\MLDatasets\n",
      "DIR_PRETRAINED                 F:\\PretrainedModels\n",
      "DIR_ROOT                       F:\\\n",
      "DIR_TRAINING                   F:\\models_newshapes\n",
      "FCN_TRAINING_PATH              F:\\models_newshapes\\train_fcn8_l2_newshapes\n",
      "FCN_VGG16_MODEL_PATH           F:\\PretrainedModels\\fcn_vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "MRCNN_TRAINING_PATH            F:\\models_newshapes\\train_mrcnn_newshapes\n",
      "RESNET_MODEL_PATH              F:\\PretrainedModels\\resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "SHAPES_MODEL_PATH              F:\\PretrainedModels\\mask_rcnn_shapes.h5\n",
      "VGG16_MODEL_PATH               F:\\PretrainedModels\\vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\n",
      ">>> Initialize ModelBase model \n",
      "   Mode      :  inference\n",
      "   Model dir :  F:\\models_newshapes\\train_fcn8_l2_newshapes\n",
      ">>> ModelBase initialiation complete\n",
      ">>> Initialize FCN model, mode:  inference architecture:  FCN8L2\n",
      "    arch set to FCN8 - with L2 Regularization\n",
      "<function fcn8_l2_graph at 0x0000001266B1BEA0>\n",
      "\n",
      "\n",
      "---------------------------------------------------\n",
      " Build FCN Model -  Arch:  FCN8L2  mode:  inference\n",
      "---------------------------------------------------\n",
      "\n",
      "------------------------------------------------------\n",
      ">>> FCN8L2 Layer With Regularization - mode: inference\n",
      "------------------------------------------------------\n",
      "     feature map      : (?, 128, 128, 7)\n",
      "     height : 128 width : 128 classes : 7\n",
      "     image_data_format:  channels_last\n",
      "     rois_per_class   :  channels_last\n",
      "     FCN L2 weight decay :  0.0001\n",
      "     Set learning phase to : 0\n",
      "   Input feature map                   :  (?, 128, 128, 7)\n",
      "   FCN Block 11 shape is               :  (None, 128, 128, 64)\n",
      "   FCN Block 12 shape is               :  (None, 128, 128, 64)\n",
      "   FCN Block 13 (Max pooling) shape is :  (None, 64, 64, 64)\n",
      "   FCN Block 21 shape is               :  (?, 64, 64, 128)\n",
      "   FCN Block 22 shape is               :  (None, 64, 64, 128)\n",
      "   FCN Block 23 (Max pooling) shape is :  (None, 32, 32, 128)\n",
      "   FCN Block 31 shape is               :  (None, 32, 32, 256)\n",
      "   FCN Block 32 shape is               :  (None, 32, 32, 256)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   FCN Block 33 shape is               :  (None, 32, 32, 256)\n",
      "   FCN Block 34 (Max pooling) shape is :  (?, 16, 16, 256)\n",
      "   FCN Block 41 shape is               :  (None, 16, 16, 512)\n",
      "   FCN Block 42 shape is               :  (None, 16, 16, 512)\n",
      "   FCN Block 43 shape is               :  (None, 16, 16, 512)\n",
      "   FCN Block 44 (Max pooling) shape is :  (?, 8, 8, 512)\n",
      "   FCN Block 51 shape is               :  (None, 8, 8, 512)\n",
      "   FCN Block 52 shape is               :  (None, 8, 8, 512)\n",
      "   FCN Block 53 shape is               :  (None, 8, 8, 512)\n",
      "   FCN Block 54 (Max pooling) shape is :  (None, 4, 4, 512)\n",
      "\n",
      "   --- FCN32 ----------------------------\n",
      "   FCN fully connected 1 (fc1) shape   :  (None, 4, 4, 4096)\n",
      "***** Call to Dropout Layer : Training is :  None\n",
      "***** in_train_phase() : Use_learning_phase:  True\n",
      "   FCN fully connected 2 (fc2) shape   :  (None, 4, 4, 4096)\n",
      "***** Call to Dropout Layer : Training is :  None\n",
      "***** in_train_phase() : Use_learning_phase:  True\n",
      "   FCN conv2d (fcn32_deconv2D) shape   :  (?, 4, 4, 7)  keras_tensor  True\n",
      "\n",
      "   --- FCN16 ----------------------------\n",
      "   FCN scorePool4 (Conv2D(Pool4)) shape is                   :  (None, 8, 8, 7)    keras_tensor  True\n",
      "   FCN 2x Upsampling (Deconvolution2D(fcn32_classify)) shape :  (None, 10, 10, 7)    keras_tensor  True\n",
      "   FCN 2x Upsampling/Cropped (Cropped2D(score2)) shape       :  (None, 8, 8, 7)    keras_tensor  True\n",
      "   FCN Add Score2,scorePool4 Add(score2_c, scorePool4) shape :  (None, 8, 8, 7)    keras_tensor  True\n",
      "   FCN upscore_pool4 (Deconv(fuse_Pool4)) shape              :  (None, 16, 16, 7)    keras_tensor  True\n",
      "\n",
      "   --- FCN8 ----------------------------\n",
      "   FCN scorePool4 (Conv2D(Pool4)) shape                      :  (None, 16, 16, 7)    keras_tensor  True\n",
      "   FCN 2x Upsampling/Cropped (Cropped2D(score2)) shape       :  (None, 16, 16, 7)    keras_tensor  True\n",
      "   FCN Add Score2,scorePool4 shape is                        :  (None, 16, 16, 7)    keras_tensor  True\n",
      "    FCN fcn8_classify/heatmap  (Deconv(fuse_Pool4)):  shape: (?, 128, 128, 7)      KB.shape:(None, 128, 128, 7)   Keras Tensor: True\n",
      "    fcn_hm (final)                 :  shape: (?, 128, 128, 7)      KB.shape:(None, 128, 128, 7)   Keras Tensor: True\n",
      "\n",
      "    fcn8_softmax                   :  shape: (?, 128, 128, 7)      KB.shape:(None, 128, 128, 7)   Keras Tensor: True\n",
      "    fcn_sm (final)                 :  shape: (?, 128, 128, 7)      KB.shape:(None, 128, 128, 7)   Keras Tensor: True\n",
      "\n",
      "\n",
      " \n",
      "----------------------\n",
      ">>> FCN Scoring Layer - mode: inference\n",
      "----------------------\n",
      "    in_heatmap.shape               :  shape: (?, 128, 128, 7)      KB.shape:(None, 128, 128, 7)   Keras Tensor: True\n",
      "    pr_hm_scores.shape             :  shape: (?, 7, 64, 24)        KB.shape:(None, 7, 64, 24)     Keras Tensor: True\n",
      "    detctions_per_image :  64 pr_scores shape (?, 7, 64, 24)\n",
      "    rois_per_image      :  64\n",
      "    config.DETECTION_MAX_INSTANCES   :  64\n",
      "    config.DETECTIONS_PER_CLASS      :  64\n",
      "    sequence_column                  :  7\n",
      "    norm_score_column                :  8\n",
      "    in_heatmap                     :  shape: (?, 128, 128, 7)      KB.shape:(None, 128, 128, 7)   Keras Tensor: True\n",
      "    pr_scores.shape                :  shape: (?, 7, 64, 24)        KB.shape:(None, 7, 64, 24)     Keras Tensor: True\n",
      "    pt2_sum shape                  :  shape: (?, 7, 64)            KB.shape:(None, 7, 64)         Keras Tensor: False\n",
      "    pt2_ind shape                  :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "    pt2_dense shape                :  shape: (?, 24)               KB.shape:(None, 24)            Keras Tensor: False\n",
      "    hm_indices                     :  shape: (?, 2)                KB.shape:(None, 2)             Keras Tensor: False\n",
      "    pt2_heatmaps                   :  shape: (?, 7, 128, 128)      KB.shape:(None, 7, 128, 128)   Keras Tensor: False\n",
      "    pt2_heatmaps                   :  shape: (?, 128, 128)         KB.shape:(None, 128, 128)      Keras Tensor: False\n",
      "    old_style_scores               :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "    alt_scores_1                   :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "    alt_scores_1_scattered         :  shape: (1, 7, 64, 3)         KB.shape:(1, 7, 64, 3)         Keras Tensor: False\n",
      "    alt_scores_1_norm(by_class)    :  shape: (1, 7, 64, 3)         KB.shape:(1, 7, 64, 3)         Keras Tensor: False\n",
      "    alt_scores_1_norm(by_image)    :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "\n",
      "    Normalize heatmap within each class !-------------------------------------\n",
      "    in_heatmap_norm :  (?, 7, 128, 128) Keras tensor  False\n",
      "    normalizer shape   :  (?, 7, 1, 1)\n",
      "    normalized heatmap :  (?, 7, 128, 128)  Keras tensor  False\n",
      "    hm_indices shape               :  shape: (?, 2)                KB.shape:(None, 2)             Keras Tensor: False\n",
      "    pt2_heatmaps                   :  shape: (?, 128, 128)         KB.shape:(None, 128, 128)      Keras Tensor: False\n",
      "    alt_scores_2                   :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "    alt_scores_2(scattered)        :  shape: (1, 7, 64, 3)         KB.shape:(1, 7, 64, 3)         Keras Tensor: False\n",
      "    alt_scores_2_norm(by_class)    :  shape: (1, 7, 64, 3)         KB.shape:(1, 7, 64, 3)         Keras Tensor: False\n",
      "    alt_scores_2_norm(by_image)    :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "    fcn_scores_dense               :  shape: (?, 24)               KB.shape:(None, 24)            Keras Tensor: False\n",
      "    seq_ids                        :  shape: (?,)                  KB.shape:(None,)               Keras Tensor: False\n",
      "    sscatter_ids                   :  shape: (?, 2)                KB.shape:(None, 2)             Keras Tensor: False\n",
      "    fcn_scores_by_class            :  shape: (1, 7, 64, 24)        KB.shape:(1, 7, 64, 24)        Keras Tensor: False\n",
      "    complete                       \n",
      "\n",
      " \n",
      "----------------------\n",
      ">>> FCN Scoring Layer - mode: inference\n",
      "----------------------\n",
      "    in_heatmap.shape               :  shape: (?, 128, 128, 7)      KB.shape:(None, 128, 128, 7)   Keras Tensor: False\n",
      "    pr_hm_scores.shape             :  shape: (?, 7, 64, 24)        KB.shape:(None, 7, 64, 24)     Keras Tensor: False\n",
      "    detctions_per_image :  64 pr_scores shape (?, 7, 64, 24)\n",
      "    rois_per_image      :  64\n",
      "    config.DETECTION_MAX_INSTANCES   :  64\n",
      "    config.DETECTIONS_PER_CLASS      :  64\n",
      "    sequence_column                  :  7\n",
      "    norm_score_column                :  8\n",
      "    in_heatmap                     :  shape: (?, 128, 128, 7)      KB.shape:(None, 128, 128, 7)   Keras Tensor: False\n",
      "    pr_scores.shape                :  shape: (?, 7, 64, 24)        KB.shape:(None, 7, 64, 24)     Keras Tensor: False\n",
      "    pt2_sum shape                  :  shape: (?, 7, 64)            KB.shape:(None, 7, 64)         Keras Tensor: False\n",
      "    pt2_ind shape                  :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "    pt2_dense shape                :  shape: (?, 24)               KB.shape:(None, 24)            Keras Tensor: False\n",
      "    hm_indices                     :  shape: (?, 2)                KB.shape:(None, 2)             Keras Tensor: False\n",
      "    pt2_heatmaps                   :  shape: (?, 7, 128, 128)      KB.shape:(None, 7, 128, 128)   Keras Tensor: False\n",
      "    pt2_heatmaps                   :  shape: (?, 128, 128)         KB.shape:(None, 128, 128)      Keras Tensor: False\n",
      "    old_style_scores               :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "    alt_scores_1                   :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "    alt_scores_1_scattered         :  shape: (1, 7, 64, 3)         KB.shape:(1, 7, 64, 3)         Keras Tensor: False\n",
      "    alt_scores_1_norm(by_class)    :  shape: (1, 7, 64, 3)         KB.shape:(1, 7, 64, 3)         Keras Tensor: False\n",
      "    alt_scores_1_norm(by_image)    :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "\n",
      "    Normalize heatmap within each class !-------------------------------------\n",
      "    in_heatmap_norm :  (?, 7, 128, 128) Keras tensor  False\n",
      "    normalizer shape   :  (?, 7, 1, 1)\n",
      "    normalized heatmap :  (?, 7, 128, 128)  Keras tensor  False\n",
      "    hm_indices shape               :  shape: (?, 2)                KB.shape:(None, 2)             Keras Tensor: False\n",
      "    pt2_heatmaps                   :  shape: (?, 128, 128)         KB.shape:(None, 128, 128)      Keras Tensor: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    alt_scores_2                   :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "    alt_scores_2(scattered)        :  shape: (1, 7, 64, 3)         KB.shape:(1, 7, 64, 3)         Keras Tensor: False\n",
      "    alt_scores_2_norm(by_class)    :  shape: (1, 7, 64, 3)         KB.shape:(1, 7, 64, 3)         Keras Tensor: False\n",
      "    alt_scores_2_norm(by_image)    :  shape: (?, 3)                KB.shape:(None, 3)             Keras Tensor: False\n",
      "    fcn_scores_dense               :  shape: (?, 24)               KB.shape:(None, 24)            Keras Tensor: False\n",
      "    seq_ids                        :  shape: (?,)                  KB.shape:(None,)               Keras Tensor: False\n",
      "    sscatter_ids                   :  shape: (?, 2)                KB.shape:(None, 2)             Keras Tensor: False\n",
      "    fcn_scores_by_class            :  shape: (1, 7, 64, 24)        KB.shape:(1, 7, 64, 24)        Keras Tensor: False\n",
      "    complete                       \n",
      "    * fcn_heatmap shape:           :  shape: (?, 128, 128, 7)      KB.shape:(None, 128, 128, 7)   Keras Tensor: True\n",
      "    * fcn_scores shape :           :  shape: (1, 7, 64, 24)        KB.shape:(1, 7, 64, 24)        Keras Tensor: True\n",
      " ================================================================\n",
      " self.keras_model.losses :  21\n",
      "0      Tensor(\"block1_conv2/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "1      Tensor(\"block4_conv3/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "2      Tensor(\"fcn32_fc1/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "3      Tensor(\"fcn32_fc2/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "4      Tensor(\"fcn32_deconv2D/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "5      Tensor(\"block3_conv3/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "6      Tensor(\"block4_conv2/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "7      Tensor(\"block5_conv3/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "8      Tensor(\"fcn16_score_pool4/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "9      Tensor(\"fcn16_upscore_pool4/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "10      Tensor(\"fcn8_score_pool3/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "11      Tensor(\"fcn8_heatmap/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "12      Tensor(\"block1_conv1/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "13      Tensor(\"block2_conv1/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "14      Tensor(\"block3_conv2/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "15      Tensor(\"block4_conv1/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "16      Tensor(\"block5_conv1/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "17      Tensor(\"block5_conv2/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "18      Tensor(\"fcn16_score2/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "19      Tensor(\"block2_conv2/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      "20      Tensor(\"block3_conv1/weight_regularizer/add:0\", shape=(), dtype=float32)\n",
      " ================================================================\n",
      "\n",
      ">>> FCN build complete. mode:  inference\n",
      ">>> FCN initialization complete. mode:  inference\n",
      "\n",
      " FCN Configuration Parameters \n",
      " ------------------------------ \n",
      "\n",
      "Configuration Parameters:\n",
      "-------------------------\n",
      "BACKBONE_SHAPES                [[32 32]\n",
      " [16 16]\n",
      " [ 8  8]\n",
      " [ 4  4]\n",
      " [ 2  2]]\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "DETECTION_MAX_INSTANCES        64\n",
      "DETECTION_MIN_CONFIDENCE       0.1\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "DETECTION_PER_CLASS            64\n",
      "DIR_DATASET                    F:\\MLDatasets\n",
      "DIR_PRETRAINED                 F:\\PretrainedModels\n",
      "DIR_TRAINING                   F:\\models_newshapes\n",
      "EPOCHS_TO_RUN                  0\n",
      "FCN_INPUT_SHAPE                [128 128]\n",
      "GPU_COUNT                      1\n",
      "HEATMAP_SCALE_FACTOR           1\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_BUFFER                   20\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_PADDING                  True\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LAST_EPOCH_RAN                 0\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MAX_SHAPES_PER_IMAGE           15\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "MIN_SHAPES_PER_IMAGE           1\n",
      "NAME                           fcn\n",
      "NEW_LOG_FOLDER                 False\n",
      "NUM_CLASSES                    7\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "RESNET_MODEL_PATH              F:\\PretrainedModels\\resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "ROI_GT_IOU_THRESHOLD           0.2\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "ROI_PROPOSAL_AREA_THRESHOLD    0\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                100\n",
      "SYSOUT                         SCREEN\n",
      "TRAINING_PATH                  F:\\models_newshapes\\train_fcn8_l2_newshapes\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "VERBOSE                        1\n",
      "VGG16_MODEL_PATH               F:\\PretrainedModels\\fcn_vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n",
      "\n",
      " FCN IO Layers \n",
      " --------------- \n",
      " Inputs:\n",
      " -------\n",
      " index:  0    input name : input_pr_hm_norm:0                         Type: float32           Shape: (?, 128, 128, 7)\n",
      " index:  1    input name : input_pr_hm_scores:0                       Type: float32           Shape: (?, 7, 64, 24)\n",
      " Outputs:\n",
      " --------\n",
      " layer:  0    output name: fcn_heatmap_lambda/fcn_hm:0                Type: float32           Shape: (?, 128, 128, 7)\n",
      " layer:  1    output name: fcn_softmax_lambda/fcn_sm:0                Type: float32           Shape: (?, 128, 128, 7)\n",
      " layer:  2    output name: fcn_scoring/fcn_hm_scores:0                Type: float32           Shape: (1, 7, 64, 24)\n",
      "-----------------------------------------------\n",
      " Load Model with init parm: [ last ]\n",
      "-----------------------------------------------\n",
      " ---> last\n",
      "   Last file is : ('F:\\\\models_newshapes\\\\train_fcn8_l2_newshapes\\\\fcn20181224T0000', 'F:\\\\models_newshapes\\\\train_fcn8_l2_newshapes\\\\fcn20181224T0000\\\\fcn_2084.h5')\n",
      ">>> load_weights() from : F:\\models_newshapes\\train_fcn8_l2_newshapes\\fcn20181224T0000\\fcn_2084.h5\n",
      "    Weights file loaded: F:\\models_newshapes\\train_fcn8_l2_newshapes\\fcn20181224T0000\\fcn_2084.h5 \n",
      "==========================================\n",
      "FCN  MODEL Load weight file COMPLETE \n",
      "==========================================\n"
     ]
    }
   ],
   "source": [
    "mrcnn_model, fcn_model = prep.build_fcn_evaluate_pipeline_newshapes(args = args,verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "###  Print some information about the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T16:22:11.383290Z",
     "start_time": "2018-12-19T16:22:11.016742Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# print(\"\\nConfiguration Parameters:\")\n",
    "# print(\"-------------------------\")\n",
    "# for a in dir(fcn_config2):\n",
    "#     if not a.startswith(\"__\") and not callable(getattr(fcn_config2, a)):\n",
    "#         print(\"{:30} {}    {}\".format(a, getattr(fcn_config2, a) == getattr(fcn_model.config, a), getattr(fcn_config2, a), getattr(fcn_model.config, a) ))\n",
    "# print(\"\\n\")\n",
    "\n",
    "# mrcnn_model.config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T11:33:00.284155Z",
     "start_time": "2019-01-04T11:32:59.930711Z"
    }
   },
   "outputs": [],
   "source": [
    "# fcn_model.keras_model.summary()\n",
    "##'fcn_init_weights.h5',\n",
    "# DIR_WEIGHTS =  'F:/models/train_fcn8_bce/fcn20181205T0000' ### Training with LR=0.00001, MSE Loss NO L2 Regularization\n",
    "DIR_WEIGHTS =  'F:/models_newshapes/train_fcn8_l2_newshapes/fcn20181224T0000' ### Training with LR=0.00001, MSE Loss NO L2 Regularization\n",
    "# files = ['fcn_0001.h5','fcn_0027.h5','fcn_0036.h5','fcn_0051.h5','fcn_0076.h5','fcn_0106.h5','fcn_0156.h5']\n",
    "# files   = ['fcn_0104.h5','fcn_0150.h5','fcn_0205.h5','fcn_0249.h5','fcn_0293.h5','fcn_0346.h5','fcn_0419.h5']\n",
    "files   = ['fcn_0001.h5','fcn_0150.h5','fcn_0346.h5','fcn_0419.h5','fcn_0421.h5',\n",
    "           'fcn_0434.h5','fcn_0450.h5','fcn_0482.h5','fcn_0521.h5','fcn_0610.h5',\n",
    "           'fcn_0687.h5','fcn_0793.h5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T11:33:18.817240Z",
     "start_time": "2019-01-04T11:33:11.579469Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FILE_IDX = 1\n",
    "weights_path = os.path.join(DIR_WEIGHTS  , files[FILE_IDX])\n",
    "print(\"Loading weights \", weights_path)\n",
    "fcn_model.load_model_weights(weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T14:59:19.616899Z",
     "start_time": "2019-01-24T14:59:19.206608Z"
    }
   },
   "outputs": [],
   "source": [
    "# del dataset_train, dataset_val, train_generator, val_generator\n",
    "# dataset_test , test_generator   = prep_newshape_dataset( mrcnn_model.config,  1000, generator=True)\n",
    "# with open('newshapes_dataset_1000_B.pkl', 'wb') as outfile:\n",
    "#     pickle.dump(dataset_test, outfile)\n",
    "## -- OR --\n",
    "\n",
    "# with open(\"E:\\\\git_projs\\\\MRCNN3\\\\train_newshapes\\\\newshapes_test_dataset_1000.pkl\", 'rb') as infile:\n",
    "with open(\"E:\\\\git_projs\\\\MRCNN3\\\\train_newshapes\\\\newshapes_test_dataset_1000_B.pkl\", 'rb') as infile:\n",
    "    dataset_test = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T14:59:21.423181Z",
     "start_time": "2019-01-24T14:59:21.076936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n",
      "internal_class:  1 ext_cls: 1 name: None - person\n",
      "internal_class:  2 ext_cls: 2 name: None - car\n",
      "internal_class:  3 ext_cls: 3 name: None - sun\n",
      "internal_class:  4 ext_cls: 4 name: None - building\n",
      "internal_class:  5 ext_cls: 5 name: None - tree\n",
      "internal_class:  6 ext_cls: 6 name: None - cloud\n",
      "Testing Dataset Image Count: 1000\n",
      "Testing Dataset Class Count: 7\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "dataset_test.display_active_class_info()\n",
    "print(\"Testing Dataset Image Count: {}\".format(len(dataset_test.image_ids)))\n",
    "print(\"Testing Dataset Class Count: {}\".format(dataset_test.num_classes))\n",
    "print(len(dataset_test.image_ids))    \n",
    "class_names = dataset_test.class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Display some images from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T14:59:56.882361Z",
     "start_time": "2019-01-24T14:59:53.125693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABWEAAANXCAYAAABUmp3MAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQSVpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1728x1296 with 40 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_list = list(range(0,40))\n",
    "image_titles = [str(i) for i in image_list]\n",
    "images = prep.get_image_batch(dataset_test, image_list)\n",
    "visualize.display_images(images, titles = image_titles, cols = 8, width = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Display Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Load using next(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T16:13:21.156100Z",
     "start_time": "2018-12-07T16:13:18.074920Z"
    },
    "hidden": true,
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "test_batch_x, _ = next(test_generator)\n",
    "display_training_batch(dataset_test, test_batch_x)\n",
    "# dataset_train.display_annotation_info(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Load a specific image using image_id -Display image with Ground Truth bounding boxes and masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T13:36:57.563484Z",
     "start_time": "2018-12-26T13:36:56.710879Z"
    },
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IMAGE_LIST = [33]\n",
    "# del images, train_batch_x\n",
    "# train_batch_x = get_evaluate_batch(dataset_train, mrcnn_model.config, generator = train_generator )\n",
    "eval_batch = get_evaluate_batch(dataset_test, mrcnn_model.config, image_ids = IMAGE_LIST)\n",
    "\n",
    "# train_batch_x, _ =  data_gen_simulate(dataset_train, mrcnn_model.config, IMAGE_LIST)\n",
    "# visualize.display_training_batch(dataset_train, train_batch_x)\n",
    "# dataset_train.display_annotation_info(IMAGE_LIST)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Run FCN evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##  Run FCN detection pipeline on first image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Load an image using `get_evaluate_batch()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T14:39:34.534159Z",
     "start_time": "2019-01-15T14:39:34.147237Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# eval_batch = get_evaluate_batch(dataset_test, mrcnn_model.config, generator = test_generator, display = True)\n",
    "eval_batch = get_evaluate_batch(dataset_test, mrcnn_model.config, image_ids = [22], display = True)\n",
    "# eval_batch = get_evaluate_batch(dataset_test, mrcnn_model.config, display = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  Run `fcn_model.evaluate()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T14:39:57.026954Z",
     "start_time": "2019-01-15T14:39:52.371151Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fcn_results = fcn_model.evaluate(mrcnn_model, eval_batch, verbose = 0)\n",
    "print(' Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T14:40:00.943324Z",
     "start_time": "2019-01-15T14:40:00.643138Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fcn_results[0].keys()\n",
    "for i, r in enumerate(fcn_results):\n",
    "    print('\\n output ', i, '  ',sorted(r.keys()))\n",
    "    for key in sorted(r):\n",
    "        print(key.ljust(20), r[key].shape)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Display FCN detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T14:41:11.149662Z",
     "start_time": "2019-01-15T14:41:10.827415Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np_format = {}\n",
    "float_formatter = lambda x: \"%10.4f\" % x\n",
    "int_formatter   = lambda x: \"%10d\" % x\n",
    "np_format['float'] = float_formatter\n",
    "np_format['int']   = int_formatter\n",
    "np.set_printoptions(linewidth=195, precision=4, floatmode='fixed', threshold =10000, formatter = np_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T13:40:22.973368Z",
     "start_time": "2018-12-26T13:40:22.596100Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# f = fcn_results[0]\n",
    "# print(f['detections'][:20])\n",
    "# # print(f['gt_bboxes'][:20])\n",
    "# print(f['gt_class_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Display MRCNN results - 1st style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T14:41:12.629250Z",
     "start_time": "2019-01-15T14:41:12.233452Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lmt =17\n",
    "f = fcn_results[0]\n",
    "\n",
    "names  = \" \".join([ '{:>10s}'.format(class_names[i][-10:]) for i in f['class_ids'][:lmt]])\n",
    "print('            classes :', f['class_ids'][:lmt])\n",
    "print('                    : ', names)\n",
    "print('                    :', f['detection_ind'][:lmt])\n",
    "print('          bbox area :', f['pr_scores'][:lmt,10])\n",
    "print('          clip area :', f['pr_scores'][:lmt,13])\n",
    "print()\n",
    "print('        orig scores :', f['scores'][:lmt])\n",
    "print('        norm scores :', f['pr_scores'][:lmt,8])\n",
    "# print('  pr_scores[5] :', f['pr_scores'][:,5])\n",
    "\n",
    "print('-'*185)\n",
    "print('         pr_scrs[8] :', f['pr_scores'][:lmt,11])\n",
    "print('      fcn_scores[8] :', f['fcn_scores'][:lmt,11])\n",
    "print()\n",
    "print('      pr_scores[13] :', f['pr_scores'][:lmt,14])\n",
    "print('     fcn_scores[13] :', f['fcn_scores'][:lmt,14])\n",
    "print()\n",
    "\n",
    "print('      pr_scores[19] :', f['pr_scores'][:lmt,20])\n",
    "print('     fcn_scores[19] :', f['fcn_scores'][:lmt,20])\n",
    "print()\n",
    "\n",
    "print(' norm pr_scores[17] :', f['pr_scores'][:lmt,17])\n",
    "print('norm fcn_scores[17] :', f['fcn_scores'][:lmt,17])\n",
    "print()\n",
    "print(' norm pr_scores[23] :', f['pr_scores'][:lmt,23])\n",
    "print('norm fcn_scores[23] :', f['fcn_scores'][:lmt,23])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T14:41:25.615251Z",
     "start_time": "2019-01-15T14:41:25.283365Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mrcnn_model.class_pred_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  Display MRCNN scores - 2nd style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T14:41:27.711285Z",
     "start_time": "2019-01-15T14:41:27.347245Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f = fcn_results[0]\n",
    "for i, [molded_bbox, cls, scr, pr_scr, fcn_scr] in enumerate(zip(f['molded_rois'].astype(np.int), f['class_ids'],  f['scores'], f['pr_scores'], f['fcn_scores'])):\n",
    "    \n",
    "    print('{} {} {:2d}  {:.<18s}  {:5.4f} {}  '.format(i, molded_bbox, cls, dataset_test.class_names[cls], scr, fcn_scr[[4,5,6,7,8]]))\n",
    "    print('{:>86s} {}'.format(' mrcnn old style scores:  ',  pr_scr[[9,10,11]]))\n",
    "    print('{:>86s} {}'.format('   fcn old style scores:  ', fcn_scr[[9,10,11]]))\n",
    "    print()\n",
    "    print('{:>86s} {}'.format('      mrcnn alt scores1:  ', pr_scr[[12,13,14,15,16,17]]))\n",
    "    print('{:>86s} {}'.format('        fcn alt scores1:  ', fcn_scr[[12,13,14,15,16,17]]))\n",
    "    print()\n",
    "    print('{:>86s} {}'.format('          mrcnn_scores2:  ', pr_scr[[18,19,20,21,22,23]]))\n",
    "    print('{:>86s} {}'.format('            fcn_scores2:  ', fcn_scr[[18,19,20,21,22,23]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  Display MRCNN scores - 3rd style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T14:41:46.018753Z",
     "start_time": "2019-01-15T14:41:45.650991Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# r = results[0]\n",
    "print(f['detections'].shape)\n",
    "print('   old_scores: (gauss. sum over large bbox/bbox area/ gauss_sum * normlzd_score))')\n",
    "print('  alt_scores 1: (gauss. sum over small mask/mask area/ gauss_sum / mask_area):  ')\n",
    "sort_by_class_order = np.argsort(f['class_ids'])\n",
    "\n",
    "\n",
    "# for i in range(len( f['class_ids'])):\n",
    "for i in sort_by_class_order:\n",
    "#     print(i , f['rois'][i].astype(np.float), f['scores'][i], f['class_ids'][i], class_names[f['class_ids'][i]])\n",
    "#     print(i , f['detections'][i], f['class_ids'][i], class_names[f['class_ids'][i]])\n",
    "    det_type = '       --> ADDED FP ' if f['pr_scores'][i,6] == -1 else '      Original detection'\n",
    "    print(i , f['rois'][i])\n",
    "    print(i , f['pr_scores'][i,:9], f['pr_scores'][i,4], class_names[ f['pr_scores'][i,4].astype(np.int)], det_type) \n",
    "    print()\n",
    "    print(i , 'pr: old_scores [9,10,11]:  '.rjust(90), f['pr_scores'][i,9:12])    \n",
    "    print(i , '  alt_scores 1 [12 - 17]:  '.rjust(90), f['pr_scores'][i,12:18])    \n",
    "    print(i , '  alt_scores 2 [18 - 23]:  '.rjust(90), f['pr_scores'][i,18:24])    \n",
    "    print(i)\n",
    "#     print(i , f['fcn_scores'][i,:8], f['fcn_scores'][i,4], class_names[ f['fcn_scores'][i,4].astype(np.int)])    \n",
    "    print(i , 'fcn: old_scores [9,10,11]:  '.rjust(90), f['fcn_scores'][i,9:12])    \n",
    "    print(i ,  '  alt_scores 1 [12 - 17]:  '.rjust(90), f['fcn_scores'][i,12:18])    \n",
    "    print(i ,  '  alt_scores 2 [18 - 23]:  '.rjust(90), f['fcn_scores'][i,18:24])    \n",
    "    print()\n",
    "    print(i , '  old score [11]:  '.rjust(90), ' from mrcnn:{:10.4f}  from FCN: {:10.4f} '.format(f['pr_scores'][i,11],f['fcn_scores'][i,11]))\n",
    "    print(i , 'alt score 1 [14]:  '.rjust(90), ' from mrcnn:{:10.4f}  from FCN: {:10.4f} '.format(f['pr_scores'][i,14],f['fcn_scores'][i,14]))\n",
    "    print(i , 'alt score 2 [20]:  '.rjust(90), ' from mrcnn:{:10.4f}  from FCN: {:10.4f} '.format(f['pr_scores'][i,20],f['fcn_scores'][i,20]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Display detections on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T14:42:04.585245Z",
     "start_time": "2019-01-15T14:42:04.250283Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f = fcn_results[0]\n",
    "print(np.unique(f['gt_class_ids']))\n",
    "print(np.unique(f['class_ids']))\n",
    "print('Image Meta: ', f['image_meta'][:10])\n",
    "visualize.display_instances(f['image'], f['rois'],  f['class_ids'], dataset_test.class_names, f['scores'],\n",
    "#                            only_classes=[6],\n",
    "                            title=\"MRCNN Predictions\", score_range=(0.0, 1.7))\n",
    "#  only_classes=[27],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T14:42:40.261712Z",
     "start_time": "2019-01-15T14:42:39.772570Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Display detections\n",
    "## old_score   : 11\n",
    "## alt_score_1 : 14\n",
    "## alt_score_2 : 20\n",
    "from mrcnn.utils import log # Display results\n",
    "# ax = visualize.get_ax(rows =1, cols = 1, size= 20)\n",
    "f = fcn_results[0]\n",
    "clses = None\n",
    "class_ids = np.unique(f['pr_scores'][:,4]).astype(int).tolist()\n",
    "print('class ids: ', class_ids)\n",
    "print('Image Meta: ', f['image_meta'][:10])\n",
    "# visualize.display_instances_from_prscores(f['image'], f['pr_scores'], class_names, score_range=(-999, 9999), only_classes = [1])\n",
    "# visualize.display_instances(f['image'], f['rois'],  f['class_ids'], class_names, f['scores'], title=\"Predictions\", score_range=(0.7, 0.99), size = 24)\n",
    "# visualize.display_instances(f['image'], f['rois'], f['class_ids'], class_names, f['scores'], title=\"Predictions - Orig Score\", score_range=(0.0, 1.99), size = 24, only_classes=clses)\n",
    "visualize.display_instances_two_scores(f['image'], f['rois'], f['class_ids'], class_names, f['pr_scores'][:,11], f['fcn_scores'][:,11],title=\"Predictions - Alt Score 0 \", only_classes=clses)\n",
    "visualize.display_instances_two_scores(f['image'], f['rois'], f['class_ids'], class_names, f['pr_scores'][:,14], f['fcn_scores'][:,14],title=\"Predictions - Alt Score 1 \", only_classes=clses)\n",
    "visualize.display_instances_two_scores(f['image'], f['rois'], f['class_ids'], class_names, f['pr_scores'][:,20], f['fcn_scores'][:,20],title=\"Predictions - Alt Score 2 \", only_classes=clses)\n",
    "# visualize.display_instances_two_scores(f['image'], f['rois'], f['class_ids'], class_names, f['pr_scores'][:,17], f['fcn_scores'][:,17],title=\"Predictions - Alt Score 1 (Normalized by class)\", only_classes=clses)\n",
    "# visualize.display_instances_two_scores(f['image'], f['rois'], f['class_ids'], class_names, f['pr_scores'][:,23], f['fcn_scores'][:,23],title=\"Predictions - Alt Score 2 (Normalized by class)\", only_classes=clses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Display Img with GT bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-01T19:33:16.664650Z",
     "start_time": "2019-01-01T19:33:16.091143Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visualize.display_image_gt(dataset_test, mrcnn_model.config, [33] ,\n",
    "#                            only_classes=[44,46] \n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Compute mAP and Display Precision/Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T14:43:24.287183Z",
     "start_time": "2019-01-15T14:43:23.386553Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visualize.plot_precision_recall_compare(fcn_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-26T13:42:20.710922Z",
     "start_time": "2018-12-26T13:42:20.083477Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_score = 5\n",
    "norm_score = 8\n",
    "alt_scr_0  = 11\n",
    "alt_scr_1  = 14   # in MRCNN alt_scr_1 ans alt_scr_2 are the same\n",
    "alt_scr_2  = 20\n",
    "\n",
    "# # Draw precision-recall curve\n",
    "f = fcn_results[0]\n",
    "AP, precisions, recalls, overlaps = utils.compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['scores'])\n",
    "visualize.plot_precision_recall(AP, precisions, recalls,ttl = \"- Orig Score\")\n",
    "# AP, precisions, recalls, overlaps = utils.compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['pr_scores'][:,norm_score])\n",
    "# visualize.plot_precision_recall(AP, precisions, recalls, ttl = \"- Normlzd Score\")\n",
    "\n",
    "# AP, precisions, recalls, overlaps = utils.compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['pr_scores'][:,alt_scr_0])\n",
    "# visualize.plot_precision_recall(AP, precisions, recalls, ttl = \"- RCNN Alt Score 0 \")\n",
    "# AP, precisions, recalls, overlaps = utils.compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['fcn_scores'][:,alt_scr_0])\n",
    "# visualize.plot_precision_recall(AP, precisions, recalls, ttl = \"- FCN Alt Score 0 \")\n",
    "\n",
    "# AP, precisions, recalls, overlaps = utils.compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['pr_scores'][:,alt_scr_1])\n",
    "# visualize.plot_precision_recall(AP, precisions, recalls, ttl = \"- RCNN Alt Score 1 \")\n",
    "# AP, precisions, recalls, overlaps = utils.compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['fcn_scores'][:,alt_scr_1])\n",
    "# visualize.plot_precision_recall(AP, precisions, recalls, ttl = \"- FCN Alt Score 1 \")\n",
    "\n",
    "\n",
    "# AP, precisions, recalls, overlaps = utils.compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['pr_scores'][:,alt_scr_2])\n",
    "# visualize.plot_precision_recall(AP, precisions, recalls, ttl = \"- RCNN Alt Score 1 \")\n",
    "# AP, precisions, recalls, overlaps = utils.compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['fcn_scores'][:,alt_scr_2])\n",
    "# visualize.plot_precision_recall(AP, precisions, recalls, ttl = \"- FCN Alt Score 1 \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##  Run MRCNN detection pipeline on second image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T12:18:54.471834Z",
     "start_time": "2018-12-11T12:18:52.684212Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mrcnn.prep_notebook import get_evaluate_batch\n",
    "# from mrcnn.prep_notebook import get_training_batch, get_inference_batch, get_evaluate_batch\n",
    "IMAGE_LIST = [4745]\n",
    "# eval_batch = get_evaluate_batch(dataset_test, mrcnn_model.config, generator = test_generator, display = True)\n",
    "eval_batch = get_evaluate_batch(dataset_test, mrcnn_model.config, IMAGE_LIST, display = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T12:36:38.317007Z",
     "start_time": "2018-12-11T12:35:53.041731Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fcn_results = fcn_model.evaluate(mrcnn_model, eval_batch, verbose =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Display detections on Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T12:20:39.014137Z",
     "start_time": "2018-12-11T12:20:38.010089Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f = fcn_results[0]\n",
    "print('Image Meta: ', f['image_meta'][:10])\n",
    "visualize.display_instances(f['image'], f['rois'],  f['class_ids'], dataset_test.class_names, f['scores'], title=\"MRCNN Predictions\", score_range=(0.0, 0.7))\n",
    "#  only_classes=[27],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T12:23:55.509980Z",
     "start_time": "2018-12-11T12:23:52.219978Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Display detections\n",
    "## old_score   : 11\n",
    "## alt_score_1 : 14\n",
    "## alt_score_2 : 20\n",
    "from mrcnn.utils import log # Display results\n",
    "# ax = visualize.get_ax(rows =1, cols = 1, size= 20)\n",
    "r = fcn_results[0]\n",
    "clses = [42]\n",
    "class_ids = np.unique(r['pr_scores'][:,4]).astype(int).tolist()\n",
    "print('class ids: ', class_ids)\n",
    "print('Image Meta: ', r['image_meta'][:10])\n",
    "\n",
    "# visualize.display_instances(r['image'], r['rois'],  r['class_ids'], class_names, r['scores'], title=\"Predictions\", score_range=(0.7, 0.99), size = 24)\n",
    "visualize.display_instances(r['image'], r['rois'],  r['class_ids'], class_names, r['scores'], title=\"Predictions\", score_range=(0.0, 0.99), only_classes= [42], size = 24)\n",
    "# visualize.display_instances_two_scores(r['image'], r['rois'], r['class_ids'], class_names, r['pr_scores'][:,11], r['fcn_scores'][:,11],title=\"Predictions\", only_classes=clses,size = 24)\n",
    "visualize.display_instances_two_scores(r['image'], r['rois'], r['class_ids'], class_names, r['pr_scores'][:,14], r['fcn_scores'][:,14],title=\"Predictions\", only_classes=clses,size = 24)\n",
    "visualize.display_instances_two_scores(r['image'], r['rois'], r['class_ids'], class_names, r['pr_scores'][:,20], r['fcn_scores'][:,20],title=\"Predictions\", only_classes=clses,size = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Compute mAP and Display Precision/Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T12:26:15.987861Z",
     "start_time": "2018-12-11T12:26:15.268137Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "old_score   = 11\n",
    "alt_score_1 = 14\n",
    "alt_score_2 = 20\n",
    "# Draw precision-recall curve\n",
    "f = fcn_results[0]\n",
    "AP, precisions, recalls, overlaps = utils.compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['scores'])\n",
    "visualize.plot_precision_recall(AP, precisions, recalls)\n",
    "AP, precisions, recalls, overlaps = utils.compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['pr_scores'][:,alt_score_1])\n",
    "visualize.plot_precision_recall(AP, precisions, recalls)\n",
    "AP, precisions, recalls, overlaps = utils.compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['fcn_scores'][:,alt_score_1])\n",
    "visualize.plot_precision_recall(AP, precisions, recalls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##  Run MRCNN detection pipeline on third image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Load image using `get_evaluate_batch()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T13:02:00.733740Z",
     "start_time": "2018-11-29T13:01:58.602571Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 13378, 33816\n",
    "image, _ = get_inference_batch(dataset_test, mrcnn_model, 13378, display=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T13:04:45.223505Z",
     "start_time": "2018-11-29T13:04:00.600086Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "results= run_mrcnn_detection(mrcnn_model,dataset_test, image_id=13378, verbose = 0)\n",
    "r = results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T13:10:44.718273Z",
     "start_time": "2018-11-29T13:10:43.709356Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Image Meta: ', r['orig_image_meta'][:10])\n",
    "visualize.display_instances(r['image'], r['rois'],  r['class_ids'], dataset_test.class_names, r['scores'], \n",
    "                             title=\"Predictions\", score_range=(0.0, 0.7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#  Compute AP routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T15:00:58.530284Z",
     "start_time": "2018-12-11T15:00:57.311339Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from mrcnn.utils import trim_zeros, compute_overlaps\n",
    "np_format = {}\n",
    "float_formatter = lambda x: \"%10.4f\" % x\n",
    "int_formatter   = lambda x: \"%10d\" % x\n",
    "np_format['float'] = float_formatter\n",
    "np_format['int']   = int_formatter\n",
    "np.set_printoptions(linewidth=195, precision=4, floatmode='fixed', threshold =10000, formatter = np_format)\n",
    "def compute_ap(gt_boxes, gt_class_ids,\n",
    "               pred_boxes, pred_class_ids, pred_scores,\n",
    "               iou_threshold=0.5):\n",
    "    '''\n",
    "    Compute Average Precision at a set IoU threshold (default 0.5).\n",
    "\n",
    "    Returns:\n",
    "    mAP:            Mean Average Precision\n",
    "    precisions:     List of precisions at different class score thresholds.\n",
    "    recalls:        List of recall values at different class score thresholds.\n",
    "    overlaps:       [pred_boxes, gt_boxes] IoU overlaps.\n",
    "    '''\n",
    "    # Trim zero padding and sort predictions by score from high to low\n",
    "    # TODO: cleaner to do zero unpadding upstream\n",
    "    gt_boxes   = trim_zeros(gt_boxes)\n",
    "    gt_class_ids = trim_zeros(np.expand_dims(gt_class_ids, axis = -1))\n",
    "    pred_boxes = trim_zeros(pred_boxes)\n",
    "    pred_scores= pred_scores[:pred_boxes.shape[0]]\n",
    "    indices    = np.argsort(pred_scores)[::-1]   ## sort indices from largest to smallest\n",
    "#     print('arg_sort indicies:', indices)\n",
    "    pred_boxes     = pred_boxes[indices]\n",
    "    pred_class_ids = pred_class_ids[indices]\n",
    "    pred_scores    = pred_scores[indices]\n",
    "    print(gt_boxes.shape, gt_class_ids.shape)\n",
    "    print(pred_boxes.shape, pred_class_ids.shape, pred_scores.shape)\n",
    "    print('ground truth:')\n",
    "    print(np.concatenate([gt_boxes,gt_class_ids], axis = -1))    \n",
    "    print('predcitions:')\n",
    "    print(np.concatenate([pred_boxes,np.expand_dims(pred_class_ids, axis = -1), np.expand_dims(pred_scores, axis = -1)], axis = -1))\n",
    "    print(' Total predicitons: ' , len(pred_scores))\n",
    "    \n",
    "    # Compute IoU overlaps [pred_boxes, gt_boxes]\n",
    "    overlaps = compute_overlaps(pred_boxes, gt_boxes)\n",
    "    print('overlaps: ',overlaps.shape)\n",
    "    print(overlaps)\n",
    "    \n",
    "    # Loop through ground truth boxes and find matching predictions\n",
    "    match_count = 0\n",
    "    pred_match = np.zeros([pred_boxes.shape[0]])\n",
    "    gt_match   = np.zeros([gt_boxes.shape[0]])\n",
    "    \n",
    "    for i in range(len(pred_boxes)):\n",
    "        # Find best matching ground truth box\n",
    "        sorted_ixs = np.argsort(overlaps[i])[::-1]\n",
    "        for j in sorted_ixs:\n",
    "            # If ground truth box is already matched, go to next one\n",
    "            if gt_match[j] == 1:\n",
    "                continue\n",
    "            # If we reach IoU smaller than the threshold, end the loop\n",
    "            iou = overlaps[i, j]\n",
    "            if iou < iou_threshold:\n",
    "                break\n",
    "            # Do we have a match?\n",
    "            if pred_class_ids[i] == gt_class_ids[j]:\n",
    "                match_count  += 1\n",
    "                gt_match[j]   = 1\n",
    "                pred_match[i] = 1\n",
    "                break\n",
    "    print(' after overlap computation')\n",
    "    print('match count:', match_count)\n",
    "    print('pred_match :', pred_match)\n",
    "    print('  gt_match :', gt_match)\n",
    "    \n",
    "    # Compute precision and recall at each prediction box step\n",
    "    # Precision = TP / (TP+FP)      Recall = TP / (TP + FN)\n",
    "    precisions = np.cumsum(pred_match) / (np.arange(len(pred_match)) + 1)\n",
    "    recalls    = np.cumsum(pred_match).astype(np.float32) / len(gt_match)\n",
    "    \n",
    "    print(' precisions')\n",
    "    print(np.cumsum(pred_match), '/', (np.arange(len(pred_match)) + 1))\n",
    "    print(precisions)\n",
    "    print()\n",
    "    print(' recalls ')\n",
    "    print(np.cumsum(pred_match), '/', len(gt_match))\n",
    "    print(recalls)\n",
    "    print()\n",
    "    \n",
    "    # Pad with start and end values to simplify the math\n",
    "    precisions = np.concatenate([[0], precisions, [0]])\n",
    "    recalls    = np.concatenate([[0], recalls, [1]])\n",
    "    print('appended [0/0], [0/1] to front/end of precisions/recalls')\n",
    "    print(' Pr: ', precisions)\n",
    "    print(' Rc: ', recalls)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # Ensure precision values decrease but don't increase. This way, the\n",
    "    # precision value at each recall threshold is the maximum it can be\n",
    "    # for all following recall thresholds, as specified by the VOC paper.\n",
    "    for i in range(len(precisions) - 2, -1, -1):\n",
    "        precisions[i] = np.maximum(precisions[i], precisions[i + 1])\n",
    "    print('backtracking precsiosn values:', list(range(len(precisions) - 2, -1, -1)))\n",
    "    print(' Pr: ', precisions)\n",
    "    print(' Rc: ', recalls)\n",
    "    print()\n",
    "    \n",
    "    # Compute mean AP over recall range\n",
    "    print(' Rc[:-1] : ', recalls[:-1])\n",
    "    print(' Rc[1:]  : ', recalls[1:])\n",
    "    print(' Pr[1:]  : ', precisions[1:])\n",
    "    print()\n",
    "\n",
    "    ## find points where recall values changes \n",
    "    indices = np.where(recalls[:-1] != recalls[1:])[0] + 1\n",
    "    mAP     = np.sum((recalls[indices] - recalls[indices - 1]) * precisions[indices])\n",
    "    \n",
    "    print('where recalls[:-1] != recalls[1:]:', np.where(recalls[:-1] != recalls[1:]))\n",
    "    print('indices at recall changes : ', indices)\n",
    "    print(' A1: recall[indices]  : ', recalls[indices])\n",
    "    print(' A2: recall[indices-1]: ', recalls[indices-1])\n",
    "    print(' A1 - A2              : ', (recalls[indices] - recalls[indices - 1]))\n",
    "    print(' P1:precision[indices]: ', precisions[indices])\n",
    "    print(' (A1-A2)*P1           : ', (recalls[indices] - recalls[indices - 1]) * precisions[indices])\n",
    "    print(' Sum = mAP            : ', mAP)\n",
    "    print(' Pr        : ', precisions)\n",
    "    print(' Rc        : ', recalls)\n",
    "    print(' Pr*Rc     : ', precisions*recalls)\n",
    "    print(' Sum(Pr*Rc): ', np.sum(precisions*recalls))\n",
    "    print(' Sum(Pr*Rc)/m: ', np.sum(precisions*recalls)/len(precisions))\n",
    "    return mAP, precisions, recalls, overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T15:02:26.112999Z",
     "start_time": "2018-12-11T15:02:25.625306Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "orig_score = 5\n",
    "norm_score = 8\n",
    "alt_scr_0  = 11\n",
    "alt_scr_1  = 14   # in MRCNN alt_scr_1 ans alt_scr_2 are the same\n",
    "alt_scr_2  = 20\n",
    "\n",
    "# AP, precisions, recalls, overlaps = compute_ap(gt_data[0]['gt_bbox'], gt_data[0]['gt_class_id'], r['molded_rois'], r['class_ids'], r['scores'])\n",
    "f = fcn_results[0]\n",
    "print(f['gt_bboxes'].shape, f['gt_class_ids'].shape)\n",
    "AP, precisions, recalls, overlaps = compute_ap(f['gt_bboxes'], f['gt_class_ids'], f['molded_rois'], f['class_ids'], f['pr_scores'][:,orig_score])\n",
    "visualize.plot_precision_recall(AP, precisions, recalls,ttl = \"- Orig Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T12:19:47.578623Z",
     "start_time": "2018-11-29T12:19:47.227152Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(precisions)\n",
    "precisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T17:55:31.579641Z",
     "start_time": "2018-11-29T17:55:31.193366Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f = fcn_results[0]\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Display Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T15:51:08.495307Z",
     "start_time": "2019-01-02T15:51:08.179586Z"
    },
    "hidden": true,
    "hideCode": false,
    "hideOutput": false
   },
   "outputs": [],
   "source": [
    "# from mrcnn.visualize import (plot_one_bbox_heatmap, \n",
    "#                              plot_3d_heatmap, plot_2d_heatmap, \n",
    "#                               plot_2d_heatmap_compare, plot_3d_heatmap_compare)\n",
    "# import matplotlib as plt\n",
    "img_id = 0\n",
    "f= fcn_results[img_id]\n",
    "image_id=f['image_meta'][0]\n",
    "print('Image id: ',image_id, ' Coco ID: ', dataset_test.image_info[image_id]['id'])\n",
    "print(f['pr_scores'][:,4])\n",
    "\n",
    "coco_class_names = dataset_test.class_names\n",
    "gt_class_ids = np.unique(f['gt_class_ids']).astype(int).tolist()\n",
    "pr_class_ids = np.unique(f['pr_scores'][:,4]).astype(int).tolist()\n",
    "fcn_class_ids = np.unique(f['fcn_scores'][:,4]).astype(int).tolist()\n",
    "print('  GT class ids: ', gt_class_ids)\n",
    "print('Pred class ids: ', pr_class_ids)\n",
    "print('FCN  class ids: ', fcn_class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Overlay images with heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Overlay image with pred_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-01T19:48:03.461028Z",
     "start_time": "2019-01-01T19:48:00.580047Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mrcnn.visualize import display_heatmaps, display_heatmaps_fcn, display_heatmaps_mrcnn, inference_heatmaps_display\n",
    "# visualize.display_image_bw(image)\n",
    "# print(model_gt_heatmap_scores.shape)\n",
    "class_ids = np.unique(f['pr_scores'][:,4]).astype(int).tolist()\n",
    "print('class ids: ', class_ids)\n",
    "inference_heatmaps_display(fcn_results, 0, class_ids = class_ids, class_names = coco_class_names, hm = 'pr_hm' ,config = mrcnn_model.config, scaling = 'class', columns = 4) \n",
    "# , display_heatmaps_mrcnn(train_batch_x, model_output, 0, hm = 'pr',  \n",
    "#                      config = mrcnn_config, class_ids = class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Overlay image with fcn_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-01T19:49:30.224424Z",
     "start_time": "2019-01-01T19:49:26.685449Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mrcnn.visualize import display_heatmaps, display_heatmaps_fcn, display_heatmaps_mrcnn, inference_heatmaps_display\n",
    "# visualize.display_image_bw(image)\n",
    "# print(model_gt_heatmap_scores.shape)\n",
    "class_ids = np.unique(f['pr_scores'][:,4]).astype(int).tolist()\n",
    "print('class ids: ', class_ids)\n",
    "inference_heatmaps_display(fcn_results, 0, \n",
    "#                            class_ids = class_ids, \n",
    "                           class_names = coco_class_names, hm = 'fcn_hm' ,config = mrcnn_model.config, scaling = 'all', columns = 4) \n",
    "# , display_heatmaps_mrcnn(train_batch_x, model_output, 0, hm = 'pr',  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Overlay image with pr_heatmaps and fcn_heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-01T19:50:57.763079Z",
     "start_time": "2019-01-01T19:50:51.890057Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize.inference_heatmaps_compare(fcn_results, image_id = 0 , hm = 'fcn_hm', \n",
    "                     config = mrcnn_model.config, \n",
    "#                      class_ids = pr_class_ids, \n",
    "                     class_names = coco_class_names, scaling = 'clip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2D Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  2D plot of `model_gt_heatmap`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T16:20:27.226627Z",
     "start_time": "2019-01-02T16:20:26.898714Z"
    },
    "hidden": true,
    "hideCode": false,
    "hideOutput": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Image : {}  ClassIds: {}'.format(img_id, gt_class_ids))\n",
    "visualize.plot_2d_heatmap(model_gt_heatmap, model_gt_heatmap_scores, img_id, gt_class_ids, \n",
    "                 class_names = coco_class_names, columns = 3, scale = 4, scaling = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  2D plot of `pr_heatmap` (w/ Ground Truth Bounding Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T16:20:48.619579Z",
     "start_time": "2019-01-02T16:20:47.525021Z"
    },
    "hidden": true,
    "hideCode": false,
    "hideOutput": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Image : {}  ClassIds: {}'.format(img_id, pr_class_ids))\n",
    "_ = visualize.plot_2d_heatmap(f['pr_hm'], f['pr_hm_scores'],0, gt_class_ids, columns = 2,\n",
    "                             class_names = coco_class_names, scale = 1, scaling = 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  2D plot of `fcn_heatmap` (w/ GT Bounding Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T16:21:01.208344Z",
     "start_time": "2019-01-02T16:21:00.122069Z"
    },
    "hidden": true,
    "hideCode": false,
    "hideOutput": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Image : {}  ClassIds: {}'.format(img_id, gt_class_ids))\n",
    "disp_classes = None \n",
    "\n",
    "_ = visualize.plot_2d_heatmap(f['fcn_hm'], f['fcn_hm_scores'],img_id, disp_classes, columns = 2, size=(9,9),\n",
    "                             class_names = coco_class_names, scale = 1, scaling = 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  2D plot of `model_fcn_heatmap` (w/ MRCNN PREDICTED Bounding Boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T16:21:31.931937Z",
     "start_time": "2019-01-02T16:21:31.071883Z"
    },
    "hidden": true,
    "hideCode": false,
    "hideOutput": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f['fcn_hm'].shape)\n",
    "print('Image : {}  ClassIds: {}'.format(img_id, pr_class_ids))\n",
    "fig  = visualize.plot_2d_heatmap(f['fcn_hm'], f['fcn_hm_scores'],img_id, pr_class_ids, columns = 2,\n",
    "                             class_names = coco_class_names, scale = 1, scaling = 'class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  2D plot of `model_fcn_heatmap` returned from model - ALL CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-12T13:06:10.736475Z",
     "start_time": "2018-12-12T13:06:10.302508Z"
    },
    "hidden": true,
    "hideCode": false,
    "hideOutput": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# class_ids = np.unique(model_pred_heatmap_scores[img_id,:,:,4]).astype(int).tolist()\n",
    "# print('Image : {}  Pred ClassIds: {}'.format(img_id, class_ids))\n",
    "# fig  = visualize.plot_2d_heatmap(model_fcn_heatmap, model_fcn_scores, img_id, \n",
    "#                                  columns = 5, class_names = coco_class_names, scale = 4, scaling = 'class')    \n",
    "#     fig.savefig('fcn_heatmaps_2d.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  2D comparative display `pred_heatmap` / `fcn_heatmap`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T14:48:41.492408Z",
     "start_time": "2019-01-02T14:48:36.731817Z"
    },
    "hidden": true,
    "hideCode": false,
    "hideOutput": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f['pr_hm_scores'].shape)\n",
    "for img_id in [0]: ##range(mrcnn_config.BATCH_SIZE):\n",
    "    class_ids = np.unique(f['pr_hm_scores'][:,:,4]).astype(int).tolist()\n",
    "    print('Image : {}  ClassIds: {}'.format(img_id, class_ids))\n",
    "#     class_ids = list(range(30))\n",
    "    _ = visualize.plot_2d_heatmap_compare(f['pr_hm'], f['fcn_hm'], f['pr_hm_scores'],  \n",
    "                            img_id, class_ids = class_ids, class_names = class_names, scale = 1, scaling = 'none' )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##  3D Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  3D plot of `fcn_heatmap` returned form model - classes predicted by MRCNN only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T16:25:50.432106Z",
     "start_time": "2019-01-02T16:25:49.443428Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from   mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# from importlib import reload\n",
    "# reload(plt)\n",
    "# %matplotlib notebook\n",
    "print(np.max(f['fcn_hm']), np.min(f['fcn_hm']))\n",
    "print(f['fcn_hm'].shape)\n",
    "class_ids = pr_class_ids\n",
    "class_ids = None \n",
    "# class_ids = np.arange(10)\n",
    "print('Image : {}  ClassIds: {}'.format(img_id, class_ids))\n",
    "# visualize.plot_3d_heatmap(f['fcn_hm'], 0, class_ids,class_names = class_names, scaling = 'none', size = (24,24), columns = 1)\n",
    "visualize.plot_3d_gaussian(f['fcn_hm'][:,:,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  3D plot of `fcn_softmax` returned form model - only classes in pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T16:19:18.144477Z",
     "start_time": "2019-01-02T16:19:13.955512Z"
    },
    "hidden": true,
    "hideCode": false,
    "hideOutput": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img_id in [0]: ##range(mrcnn_config.BATCH_SIZE):\n",
    "    print(f['fcn_hm'].shape)\n",
    "    class_ids = np.unique(f['pr_scores'][:,4]).astype(int).tolist()\n",
    "    print('Image : {}  ClassIds: {}'.format(img_id, class_ids))\n",
    "    visualize.plot_3d_heatmap(f['fcn_sm'], 0,class_names = coco_class_names, scaling = 'none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Run Object Detection on sample images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test on a random image from images folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T12:08:31.773553Z",
     "start_time": "2018-12-11T12:08:30.831194Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load a random image from the images folder\n",
    "# import mrcnn.visualize as visualize\n",
    "import random\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "IMAGE_DIR = 'E:/git_projs/mrcnn3/images'\n",
    "# IMAGE_DIR = '/esat/tiger/joramas/mscStudentsData/kbardool/projs/mrcnn3/images'\n",
    "file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "# print(file_names)\n",
    "random_filename = random.choice(file_names)\n",
    "print(random_filename)\n",
    "image = skimage.io.imread(os.path.join(IMAGE_DIR, random_filename))\n",
    "plt.figure(figsize=(11,11))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-11T11:37:43.343320Z",
     "start_time": "2018-12-11T11:37:42.962339Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run detection\n",
    "# results = mrcnn_model.detect([image], verbose=1)\n",
    "results = fcn_model.detect_from_images(mrcnn_model, [image], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  Test on loaded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T14:25:29.070359Z",
     "start_time": "2018-10-25T14:25:27.993958Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "r = results[0]\n",
    "for bbox, cls, scr in zip(r['rois'], r['class_ids'],  r['scores']):\n",
    "    print('{}    {:2d}    {:.<20s}  {:5.4f} '.format(bbox, cls, dataset_test.class_names[cls], scr))\n",
    "visualize.display_instances(image, r['rois'], r['class_ids'], dataset_test.class_names, r['scores']) #, score_range=(0.21, 0.22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T14:09:41.110358Z",
     "start_time": "2018-10-25T14:09:39.918627Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = results[0]\n",
    "for bbox, cls, scr in zip(r['rois'], r['class_ids'],  r['scores']):\n",
    "    print('{}    {:2d}    {:.<20s}  {:5.4f} '.format(bbox,cls,dataset_test.class_names[cls], scr))\n",
    "visualize.display_instances(image, r['rois'], r['class_ids'], dataset_test.class_names, r['scores'], ax=visualize.get_ax(size=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "###  Display random image from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-05T18:58:10.702912Z",
     "start_time": "2018-09-05T18:58:10.026402Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load random image and mask. ### 27711 persons and boats\n",
    "image_id = np.random.choice(dataset_test.image_ids)\n",
    "image    = dataset_test.load_image(image_id)\n",
    "mask, class_ids = dataset_test.load_mask(image_id)\n",
    "# Compute Bounding box\n",
    "bbox = utils.extract_bboxes(mask)\n",
    "\n",
    "# Display image and additional stats\n",
    "print(\"image_id \", image_id, dataset_test.image_reference(image_id))\n",
    "log(\"image\", image)\n",
    "log(\"mask\", mask)\n",
    "log(\"class_ids\", class_ids)\n",
    "log(\"bbox\", bbox)\n",
    "print(class_ids.shape[0], bbox.shape[0])\n",
    "# Display image and instances\n",
    "# visualize.display_instances_with_mask(image, bbox, mask, class_ids, dataset_train.class_names)\n",
    "visualize.display_instances(image, bbox, class_ids, dataset_test.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-05T18:58:50.331159Z",
     "start_time": "2018-09-05T18:58:49.583165Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = mrcnn_model.detect([image], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-05T18:58:57.782512Z",
     "start_time": "2018-09-05T18:58:57.228345Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('resutls :', results)\n",
    "# Visualize results\n",
    "r = results[0]\n",
    "for bbox, cls, scr in zip(r['rois'], r['class_ids'],  r['scores']):\n",
    "    print('{}    {:2d}    {:.<20s}  {:5.4f} '.format(bbox,cls,dataset_test.class_names[cls], scr))\n",
    "visualize.display_instances(image, r['rois'], r['class_ids'], dataset_test.class_names, r['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Test on a random image from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-05T18:59:32.254388Z",
     "start_time": "2018-09-05T18:59:31.391971Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "# Validation dataset\n",
    "# dataset_val = shapes.ShapesDataset()\n",
    "# dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "# dataset_val.prepare()\n",
    "\n",
    "image_id = random.choice(dataset_test.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox =\\\n",
    "    load_image_gt(dataset_test, mrcnn_config, image_id, use_mini_mask=False)\n",
    "    \n",
    "\n",
    "print('Image Id :', image_id)    \n",
    "shape_list = dataset_test.image_info[image_id] \n",
    "pp.pprint(shape_list)\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "print(image_meta)\n",
    "log(\"gt_class_id\", gt_bbox)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "# log(\"gt_mask\", gt_mask)\n",
    "\n",
    "print(\" 1: person   2: car  3: sun  4: building  5: tree  6: cloud \")\n",
    "visualize.display_instances(original_image, gt_bbox, gt_class_id, \n",
    "                            dataset_test.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T11:56:18.825541Z",
     "start_time": "2018-05-10T11:56:16.622430Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T11:56:20.774651Z",
     "start_time": "2018-05-10T11:56:20.439754Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "r = results[0]\n",
    "print('  rois       : ', r['rois'])\n",
    "print('  masks      : ', r['masks'].shape)\n",
    "print('  class ids  : ', r['class_ids'])\n",
    "print('  class names: ', dataset_test.class_names)\n",
    "print('  scores     : ', r['scores'])\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_test.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of mAP  over a number of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T15:00:23.970643Z",
     "start_time": "2019-01-24T15:00:22.789756Z"
    }
   },
   "outputs": [],
   "source": [
    "np_format = {}\n",
    "np_format['float'] = lambda x: \"%10.4f\" % x\n",
    "np_format['int']   = lambda x: \"%10d\" % x\n",
    "np.set_printoptions(linewidth=195, precision=4, floatmode='fixed', threshold =10000, formatter = np_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load previously saved AP Results from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:38:34.290594Z",
     "start_time": "2019-02-03T14:38:33.938345Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: E:\\git_projs\\MRCNN3\\train_newshapes\\eval_method1_results      Old Filename:  eval1_AP_results_2019_01_17 New Filename: eval1_AP_results_2019_01_24\n"
     ]
    }
   ],
   "source": [
    "####eval_method = '1'\n",
    "save_path = \"E:\\\\git_projs\\\\MRCNN3\\\\train_newshapes\\\\eval_method\"+eval_method+\"_results\"\n",
    "file_prefix = 'eval'+eval_method\n",
    "old_AP_results_file = file_prefix+'_AP_results_2019_01_17'\n",
    "new_AP_results_file = file_prefix+'_AP_results_2019_01_24'\n",
    "print('Path:' ,save_path, '     Old Filename: ', old_AP_results_file,  'New Filename:', new_AP_results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:38:38.001239Z",
     "start_time": "2019-02-03T14:38:37.671003Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\git_projs\\\\MRCNN3\\\\train_newshapes\\\\eval_method1_results\\\\eval1_AP_results_2019_01_17.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c6da024f398c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mAll_APResults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m##--OR --#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mold_AP_results_file\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0moutfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mAPRes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mAll_APResults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAPRes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\git_projs\\\\MRCNN3\\\\train_newshapes\\\\eval_method1_results\\\\eval1_AP_results_2019_01_17.pkl'"
     ]
    }
   ],
   "source": [
    "All_APResults = {}\n",
    "##--OR --#\n",
    "with open(os.path.join(save_path, old_AP_results_file+'.pkl'), 'rb') as outfile:\n",
    "    APRes = pickle.load(outfile)\n",
    "All_APResults = APRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T15:00:52.999206Z",
     "start_time": "2019-01-24T15:00:52.708000Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# print('/'.join(weights_file.split('/')[-3:]))\n",
    "print(len(All_APResults.keys()))\n",
    "for i in sorted(All_APResults):\n",
    "    print(i, All_APResults[i]['Epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load weight file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T15:01:13.335661Z",
     "start_time": "2019-01-24T15:01:13.005412Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fcn_0001.h5\n"
     ]
    }
   ],
   "source": [
    "DIR_WEIGHTS =  'F:/models_newshapes/train_fcn8_l2_newshapes/fcn20181224T0000' ### Training with LR=0.00001, MSE Loss NO L2 Regularization\n",
    "\n",
    "files   = ['fcn_0001.h5', 'fcn_0150.h5', 'fcn_0346.h5', 'fcn_0421.h5',\n",
    "           'fcn_0450.h5', 'fcn_0482.h5', 'fcn_0521.h5', 'fcn_0610.h5',\n",
    "           'fcn_0687.h5', 'fcn_0793.h5', 'fcn_0821.h5', 'fcn_0940.h5',\n",
    "           'fcn_1012.h5', 'fcn_1127.h5', 'fcn_1644.h5', 'fcn_1776.h5',\n",
    "           'fcn_1848.h5', 'fcn_2017.h5', 'fcn_2084.h5', 'fcn_x0434.h5', 'fcn_x0419.h5']\n",
    "\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Run `compute_ap` over images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T15:24:46.813655Z",
     "start_time": "2019-01-24T15:02:52.315495Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights  F:/models_newshapes/train_fcn8_l2_newshapes/fcn20181224T0000\\fcn_0150.h5\n",
      "-----------------------------------------------\n",
      " Load Model with init parm: [ F:/models_newshapes/train_fcn8_l2_newshapes/fcn20181224T0000\\fcn_0150.h5 ]\n",
      "-----------------------------------------------\n",
      " ---> Explicit weight file\n",
      ">>> load_weights() from : F:/models_newshapes/train_fcn8_l2_newshapes/fcn20181224T0000\\fcn_0150.h5\n",
      "    Weights file loaded: F:/models_newshapes/train_fcn8_l2_newshapes/fcn20181224T0000\\fcn_0150.h5 \n",
      "==========================================\n",
      "FCN  MODEL Load weight file COMPLETE \n",
      "==========================================\n",
      "500\n",
      "==> Calculate AP for image_id :  0\n",
      "==> Calculate AP for image_id :  1\n",
      "==> Calculate AP for image_id :  2\n",
      "==> Calculate AP for image_id :  3\n",
      "==> Calculate AP for image_id :  4\n",
      "==> Calculate AP for image_id :  5\n",
      "==> Calculate AP for image_id :  6\n",
      "==> Calculate AP for image_id :  7\n",
      "==> Calculate AP for image_id :  8\n",
      "==> Calculate AP for image_id :  9\n",
      "==> Calculate AP for image_id :  10\n",
      "==> Calculate AP for image_id :  11\n",
      "==> Calculate AP for image_id :  12\n",
      "==> Calculate AP for image_id :  13\n",
      "==> Calculate AP for image_id :  14\n",
      "==> Calculate AP for image_id :  15\n",
      "==> Calculate AP for image_id :  16\n",
      "==> Calculate AP for image_id :  17\n",
      "==> Calculate AP for image_id :  18\n",
      "==> Calculate AP for image_id :  19\n",
      "==> Calculate AP for image_id :  20\n",
      "==> Calculate AP for image_id :  21\n",
      "==> Calculate AP for image_id :  22\n",
      "==> Calculate AP for image_id :  23\n",
      "==> Calculate AP for image_id :  24\n",
      "==> Calculate AP for image_id :  25\n",
      "==> Calculate AP for image_id :  26\n",
      "==> Calculate AP for image_id :  27\n",
      "==> Calculate AP for image_id :  28\n",
      "==> Calculate AP for image_id :  29\n",
      "==> Calculate AP for image_id :  30\n",
      "==> Calculate AP for image_id :  31\n",
      "==> Calculate AP for image_id :  32\n",
      "==> Calculate AP for image_id :  33\n",
      "==> Calculate AP for image_id :  34\n",
      "==> Calculate AP for image_id :  35\n",
      "==> Calculate AP for image_id :  36\n",
      "==> Calculate AP for image_id :  37\n",
      "==> Calculate AP for image_id :  38\n",
      "==> Calculate AP for image_id :  39\n",
      "==> Calculate AP for image_id :  40\n",
      "==> Calculate AP for image_id :  41\n",
      "==> Calculate AP for image_id :  42\n",
      "==> Calculate AP for image_id :  43\n",
      "==> Calculate AP for image_id :  44\n",
      "==> Calculate AP for image_id :  45\n",
      "==> Calculate AP for image_id :  46\n",
      "==> Calculate AP for image_id :  47\n",
      "==> Calculate AP for image_id :  48\n",
      "==> Calculate AP for image_id :  49\n",
      "==> Calculate AP for image_id :  50\n",
      "==> Calculate AP for image_id :  51\n",
      "==> Calculate AP for image_id :  52\n",
      "==> Calculate AP for image_id :  53\n",
      "==> Calculate AP for image_id :  54\n",
      "==> Calculate AP for image_id :  55\n",
      "==> Calculate AP for image_id :  56\n",
      "==> Calculate AP for image_id :  57\n",
      "==> Calculate AP for image_id :  58\n",
      "==> Calculate AP for image_id :  59\n",
      "==> Calculate AP for image_id :  60\n",
      "==> Calculate AP for image_id :  61\n",
      "==> Calculate AP for image_id :  62\n",
      "==> Calculate AP for image_id :  63\n",
      "==> Calculate AP for image_id :  64\n",
      "==> Calculate AP for image_id :  65\n",
      "==> Calculate AP for image_id :  66\n",
      "==> Calculate AP for image_id :  67\n",
      "==> Calculate AP for image_id :  68\n",
      "==> Calculate AP for image_id :  69\n",
      "==> Calculate AP for image_id :  70\n",
      "==> Calculate AP for image_id :  71\n",
      "==> Calculate AP for image_id :  72\n",
      "==> Calculate AP for image_id :  73\n",
      "==> Calculate AP for image_id :  74\n",
      "==> Calculate AP for image_id :  75\n",
      "==> Calculate AP for image_id :  76\n",
      "==> Calculate AP for image_id :  77\n",
      "==> Calculate AP for image_id :  78\n",
      "==> Calculate AP for image_id :  79\n",
      "==> Calculate AP for image_id :  80\n",
      "==> Calculate AP for image_id :  81\n",
      "==> Calculate AP for image_id :  82\n",
      "==> Calculate AP for image_id :  83\n",
      "==> Calculate AP for image_id :  84\n",
      "==> Calculate AP for image_id :  85\n",
      "==> Calculate AP for image_id :  86\n",
      "==> Calculate AP for image_id :  87\n",
      "==> Calculate AP for image_id :  88\n",
      "==> Calculate AP for image_id :  89\n",
      "==> Calculate AP for image_id :  90\n",
      "==> Calculate AP for image_id :  91\n",
      "==> Calculate AP for image_id :  92\n",
      "==> Calculate AP for image_id :  93\n",
      "==> Calculate AP for image_id :  94\n",
      "==> Calculate AP for image_id :  95\n",
      "==> Calculate AP for image_id :  96\n",
      "==> Calculate AP for image_id :  97\n",
      "==> Calculate AP for image_id :  98\n",
      "==> Calculate AP for image_id :  99\n",
      "==> Calculate AP for image_id :  100\n",
      "==> Calculate AP for image_id :  101\n",
      "==> Calculate AP for image_id :  102\n",
      "==> Calculate AP for image_id :  103\n",
      "==> Calculate AP for image_id :  104\n",
      "==> Calculate AP for image_id :  105\n",
      "==> Calculate AP for image_id :  106\n",
      "==> Calculate AP for image_id :  107\n",
      "==> Calculate AP for image_id :  108\n",
      "==> Calculate AP for image_id :  109\n",
      "==> Calculate AP for image_id :  110\n",
      "==> Calculate AP for image_id :  111\n",
      "==> Calculate AP for image_id :  112\n",
      "==> Calculate AP for image_id :  113\n",
      "==> Calculate AP for image_id :  114\n",
      "==> Calculate AP for image_id :  115\n",
      "==> Calculate AP for image_id :  116\n",
      "==> Calculate AP for image_id :  117\n",
      "==> Calculate AP for image_id :  118\n",
      "==> Calculate AP for image_id :  119\n",
      "==> Calculate AP for image_id :  120\n",
      "==> Calculate AP for image_id :  121\n",
      "==> Calculate AP for image_id :  122\n",
      "==> Calculate AP for image_id :  123\n",
      "==> Calculate AP for image_id :  124\n",
      "==> Calculate AP for image_id :  125\n",
      "==> Calculate AP for image_id :  126\n",
      "==> Calculate AP for image_id :  127\n",
      "==> Calculate AP for image_id :  128\n",
      "==> Calculate AP for image_id :  129\n",
      "==> Calculate AP for image_id :  130\n",
      "==> Calculate AP for image_id :  131\n",
      "==> Calculate AP for image_id :  132\n",
      "==> Calculate AP for image_id :  133\n",
      "==> Calculate AP for image_id :  134\n",
      "==> Calculate AP for image_id :  135\n",
      "==> Calculate AP for image_id :  136\n",
      "==> Calculate AP for image_id :  137\n",
      "==> Calculate AP for image_id :  138\n",
      "==> Calculate AP for image_id :  139\n",
      "==> Calculate AP for image_id :  140\n",
      "==> Calculate AP for image_id :  141\n",
      "==> Calculate AP for image_id :  142\n",
      "==> Calculate AP for image_id :  143\n",
      "==> Calculate AP for image_id :  144\n",
      "==> Calculate AP for image_id :  145\n",
      "==> Calculate AP for image_id :  146\n",
      "==> Calculate AP for image_id :  147\n",
      "==> Calculate AP for image_id :  148\n",
      "==> Calculate AP for image_id :  149\n",
      "==> Calculate AP for image_id :  150\n",
      "==> Calculate AP for image_id :  151\n",
      "==> Calculate AP for image_id :  152\n",
      "==> Calculate AP for image_id :  153\n",
      "==> Calculate AP for image_id :  154\n",
      "==> Calculate AP for image_id :  155\n",
      "==> Calculate AP for image_id :  156\n",
      "==> Calculate AP for image_id :  157\n",
      "==> Calculate AP for image_id :  158\n",
      "==> Calculate AP for image_id :  159\n",
      "==> Calculate AP for image_id :  160\n",
      "==> Calculate AP for image_id :  161\n",
      "==> Calculate AP for image_id :  162\n",
      "==> Calculate AP for image_id :  163\n",
      "==> Calculate AP for image_id :  164\n",
      "==> Calculate AP for image_id :  165\n",
      "==> Calculate AP for image_id :  166\n",
      "==> Calculate AP for image_id :  167\n",
      "==> Calculate AP for image_id :  168\n",
      "==> Calculate AP for image_id :  169\n",
      "==> Calculate AP for image_id :  170\n",
      "==> Calculate AP for image_id :  171\n",
      "==> Calculate AP for image_id :  172\n",
      "==> Calculate AP for image_id :  173\n",
      "==> Calculate AP for image_id :  174\n",
      "==> Calculate AP for image_id :  175\n",
      "==> Calculate AP for image_id :  176\n",
      "==> Calculate AP for image_id :  177\n",
      "==> Calculate AP for image_id :  178\n",
      "==> Calculate AP for image_id :  179\n",
      "==> Calculate AP for image_id :  180\n",
      "==> Calculate AP for image_id :  181\n",
      "==> Calculate AP for image_id :  182\n",
      "==> Calculate AP for image_id :  183\n",
      "==> Calculate AP for image_id :  184\n",
      "==> Calculate AP for image_id :  185\n",
      "==> Calculate AP for image_id :  186\n",
      "==> Calculate AP for image_id :  187\n",
      "==> Calculate AP for image_id :  188\n",
      "==> Calculate AP for image_id :  189\n",
      "==> Calculate AP for image_id :  190\n",
      "==> Calculate AP for image_id :  191\n",
      "==> Calculate AP for image_id :  192\n",
      "==> Calculate AP for image_id :  193\n",
      "==> Calculate AP for image_id :  194\n",
      "==> Calculate AP for image_id :  195\n",
      "==> Calculate AP for image_id :  196\n",
      "==> Calculate AP for image_id :  197\n",
      "==> Calculate AP for image_id :  198\n",
      "==> Calculate AP for image_id :  199\n",
      "==> Calculate AP for image_id :  200\n",
      "==> Calculate AP for image_id :  201\n",
      "==> Calculate AP for image_id :  202\n",
      "==> Calculate AP for image_id :  203\n",
      "==> Calculate AP for image_id :  204\n",
      "==> Calculate AP for image_id :  205\n",
      "==> Calculate AP for image_id :  206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Calculate AP for image_id :  207\n",
      "==> Calculate AP for image_id :  208\n",
      "==> Calculate AP for image_id :  209\n",
      "==> Calculate AP for image_id :  210\n",
      "==> Calculate AP for image_id :  211\n",
      "==> Calculate AP for image_id :  212\n",
      "==> Calculate AP for image_id :  213\n",
      "==> Calculate AP for image_id :  214\n",
      "==> Calculate AP for image_id :  215\n",
      "==> Calculate AP for image_id :  216\n",
      "==> Calculate AP for image_id :  217\n",
      "==> Calculate AP for image_id :  218\n",
      "==> Calculate AP for image_id :  219\n",
      "==> Calculate AP for image_id :  220\n",
      "==> Calculate AP for image_id :  221\n",
      "==> Calculate AP for image_id :  222\n",
      "==> Calculate AP for image_id :  223\n",
      "==> Calculate AP for image_id :  224\n",
      "==> Calculate AP for image_id :  225\n",
      "==> Calculate AP for image_id :  226\n",
      "==> Calculate AP for image_id :  227\n",
      "==> Calculate AP for image_id :  228\n",
      "==> Calculate AP for image_id :  229\n",
      "==> Calculate AP for image_id :  230\n",
      "==> Calculate AP for image_id :  231\n",
      "==> Calculate AP for image_id :  232\n",
      "==> Calculate AP for image_id :  233\n",
      "==> Calculate AP for image_id :  234\n",
      "==> Calculate AP for image_id :  235\n",
      "==> Calculate AP for image_id :  236\n",
      "==> Calculate AP for image_id :  237\n",
      "==> Calculate AP for image_id :  238\n",
      "==> Calculate AP for image_id :  239\n",
      "==> Calculate AP for image_id :  240\n",
      "==> Calculate AP for image_id :  241\n",
      "==> Calculate AP for image_id :  242\n",
      "==> Calculate AP for image_id :  243\n",
      "==> Calculate AP for image_id :  244\n",
      "==> Calculate AP for image_id :  245\n",
      "==> Calculate AP for image_id :  246\n",
      "==> Calculate AP for image_id :  247\n",
      "==> Calculate AP for image_id :  248\n",
      "==> Calculate AP for image_id :  249\n",
      "==> Calculate AP for image_id :  250\n",
      "==> Calculate AP for image_id :  251\n",
      "==> Calculate AP for image_id :  252\n",
      "==> Calculate AP for image_id :  253\n",
      "==> Calculate AP for image_id :  254\n",
      "==> Calculate AP for image_id :  255\n",
      "==> Calculate AP for image_id :  256\n",
      "==> Calculate AP for image_id :  257\n",
      "==> Calculate AP for image_id :  258\n",
      "==> Calculate AP for image_id :  259\n",
      "==> Calculate AP for image_id :  260\n",
      "==> Calculate AP for image_id :  261\n",
      "==> Calculate AP for image_id :  262\n",
      "==> Calculate AP for image_id :  263\n",
      "==> Calculate AP for image_id :  264\n",
      "==> Calculate AP for image_id :  265\n",
      "==> Calculate AP for image_id :  266\n",
      "==> Calculate AP for image_id :  267\n",
      "==> Calculate AP for image_id :  268\n",
      "==> Calculate AP for image_id :  269\n",
      "==> Calculate AP for image_id :  270\n",
      "==> Calculate AP for image_id :  271\n",
      "==> Calculate AP for image_id :  272\n",
      "==> Calculate AP for image_id :  273\n",
      "==> Calculate AP for image_id :  274\n",
      "==> Calculate AP for image_id :  275\n",
      "==> Calculate AP for image_id :  276\n",
      "==> Calculate AP for image_id :  277\n",
      "==> Calculate AP for image_id :  278\n",
      "==> Calculate AP for image_id :  279\n",
      "==> Calculate AP for image_id :  280\n",
      "==> Calculate AP for image_id :  281\n",
      "==> Calculate AP for image_id :  282\n",
      "==> Calculate AP for image_id :  283\n",
      "==> Calculate AP for image_id :  284\n",
      "==> Calculate AP for image_id :  285\n",
      "==> Calculate AP for image_id :  286\n",
      "==> Calculate AP for image_id :  287\n",
      "==> Calculate AP for image_id :  288\n",
      "==> Calculate AP for image_id :  289\n",
      "==> Calculate AP for image_id :  290\n",
      "==> Calculate AP for image_id :  291\n",
      "==> Calculate AP for image_id :  292\n",
      "==> Calculate AP for image_id :  293\n",
      "==> Calculate AP for image_id :  294\n",
      "==> Calculate AP for image_id :  295\n",
      "==> Calculate AP for image_id :  296\n",
      "==> Calculate AP for image_id :  297\n",
      "==> Calculate AP for image_id :  298\n",
      "==> Calculate AP for image_id :  299\n",
      "==> Calculate AP for image_id :  300\n",
      "==> Calculate AP for image_id :  301\n",
      "==> Calculate AP for image_id :  302\n",
      "==> Calculate AP for image_id :  303\n",
      "==> Calculate AP for image_id :  304\n",
      "==> Calculate AP for image_id :  305\n",
      "==> Calculate AP for image_id :  306\n",
      "==> Calculate AP for image_id :  307\n",
      "==> Calculate AP for image_id :  308\n",
      "==> Calculate AP for image_id :  309\n",
      "==> Calculate AP for image_id :  310\n",
      "==> Calculate AP for image_id :  311\n",
      "==> Calculate AP for image_id :  312\n",
      "==> Calculate AP for image_id :  313\n",
      "==> Calculate AP for image_id :  314\n",
      "==> Calculate AP for image_id :  315\n",
      "==> Calculate AP for image_id :  316\n",
      "==> Calculate AP for image_id :  317\n",
      "==> Calculate AP for image_id :  318\n",
      "==> Calculate AP for image_id :  319\n",
      "==> Calculate AP for image_id :  320\n",
      "==> Calculate AP for image_id :  321\n",
      "==> Calculate AP for image_id :  322\n",
      "==> Calculate AP for image_id :  323\n",
      "==> Calculate AP for image_id :  324\n",
      "==> Calculate AP for image_id :  325\n",
      "==> Calculate AP for image_id :  326\n",
      "==> Calculate AP for image_id :  327\n",
      "==> Calculate AP for image_id :  328\n",
      "==> Calculate AP for image_id :  329\n",
      "==> Calculate AP for image_id :  330\n",
      "==> Calculate AP for image_id :  331\n",
      "==> Calculate AP for image_id :  332\n",
      "==> Calculate AP for image_id :  333\n",
      "==> Calculate AP for image_id :  334\n",
      "==> Calculate AP for image_id :  335\n",
      "==> Calculate AP for image_id :  336\n",
      "==> Calculate AP for image_id :  337\n",
      "==> Calculate AP for image_id :  338\n",
      "==> Calculate AP for image_id :  339\n",
      "==> Calculate AP for image_id :  340\n",
      "==> Calculate AP for image_id :  341\n",
      "==> Calculate AP for image_id :  342\n",
      "==> Calculate AP for image_id :  343\n",
      "==> Calculate AP for image_id :  344\n",
      "==> Calculate AP for image_id :  345\n",
      "==> Calculate AP for image_id :  346\n",
      "==> Calculate AP for image_id :  347\n",
      "==> Calculate AP for image_id :  348\n",
      "==> Calculate AP for image_id :  349\n",
      "==> Calculate AP for image_id :  350\n",
      "==> Calculate AP for image_id :  351\n",
      "==> Calculate AP for image_id :  352\n",
      "==> Calculate AP for image_id :  353\n",
      "==> Calculate AP for image_id :  354\n",
      "==> Calculate AP for image_id :  355\n",
      "==> Calculate AP for image_id :  356\n",
      "==> Calculate AP for image_id :  357\n",
      "==> Calculate AP for image_id :  358\n",
      "==> Calculate AP for image_id :  359\n",
      "==> Calculate AP for image_id :  360\n",
      "==> Calculate AP for image_id :  361\n",
      "==> Calculate AP for image_id :  362\n",
      "==> Calculate AP for image_id :  363\n",
      "==> Calculate AP for image_id :  364\n",
      "==> Calculate AP for image_id :  365\n",
      "==> Calculate AP for image_id :  366\n",
      "==> Calculate AP for image_id :  367\n",
      "==> Calculate AP for image_id :  368\n",
      "==> Calculate AP for image_id :  369\n",
      "==> Calculate AP for image_id :  370\n",
      "==> Calculate AP for image_id :  371\n",
      "==> Calculate AP for image_id :  372\n",
      "==> Calculate AP for image_id :  373\n",
      "==> Calculate AP for image_id :  374\n",
      "==> Calculate AP for image_id :  375\n",
      "==> Calculate AP for image_id :  376\n",
      "==> Calculate AP for image_id :  377\n",
      "==> Calculate AP for image_id :  378\n",
      "==> Calculate AP for image_id :  379\n",
      "==> Calculate AP for image_id :  380\n",
      "==> Calculate AP for image_id :  381\n",
      "==> Calculate AP for image_id :  382\n",
      "==> Calculate AP for image_id :  383\n",
      "==> Calculate AP for image_id :  384\n",
      "==> Calculate AP for image_id :  385\n",
      "==> Calculate AP for image_id :  386\n",
      "==> Calculate AP for image_id :  387\n",
      "==> Calculate AP for image_id :  388\n",
      "==> Calculate AP for image_id :  389\n",
      "==> Calculate AP for image_id :  390\n",
      "==> Calculate AP for image_id :  391\n",
      "==> Calculate AP for image_id :  392\n",
      "==> Calculate AP for image_id :  393\n",
      "==> Calculate AP for image_id :  394\n",
      "==> Calculate AP for image_id :  395\n",
      "==> Calculate AP for image_id :  396\n",
      "==> Calculate AP for image_id :  397\n",
      "==> Calculate AP for image_id :  398\n",
      "==> Calculate AP for image_id :  399\n",
      "==> Calculate AP for image_id :  400\n",
      "==> Calculate AP for image_id :  401\n",
      "==> Calculate AP for image_id :  402\n",
      "==> Calculate AP for image_id :  403\n",
      "==> Calculate AP for image_id :  404\n",
      "==> Calculate AP for image_id :  405\n",
      "==> Calculate AP for image_id :  406\n",
      "==> Calculate AP for image_id :  407\n",
      "==> Calculate AP for image_id :  408\n",
      "==> Calculate AP for image_id :  409\n",
      "==> Calculate AP for image_id :  410\n",
      "==> Calculate AP for image_id :  411\n",
      "==> Calculate AP for image_id :  412\n",
      "==> Calculate AP for image_id :  413\n",
      "==> Calculate AP for image_id :  414\n",
      "==> Calculate AP for image_id :  415\n",
      "==> Calculate AP for image_id :  416\n",
      "==> Calculate AP for image_id :  417\n",
      "==> Calculate AP for image_id :  418\n",
      "==> Calculate AP for image_id :  419\n",
      "==> Calculate AP for image_id :  420\n",
      "==> Calculate AP for image_id :  421\n",
      "==> Calculate AP for image_id :  422\n",
      "==> Calculate AP for image_id :  423\n",
      "==> Calculate AP for image_id :  424\n",
      "==> Calculate AP for image_id :  425\n",
      "==> Calculate AP for image_id :  426\n",
      "==> Calculate AP for image_id :  427\n",
      "==> Calculate AP for image_id :  428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Calculate AP for image_id :  429\n",
      "==> Calculate AP for image_id :  430\n",
      "==> Calculate AP for image_id :  431\n",
      "==> Calculate AP for image_id :  432\n",
      "==> Calculate AP for image_id :  433\n",
      "==> Calculate AP for image_id :  434\n",
      "==> Calculate AP for image_id :  435\n",
      "==> Calculate AP for image_id :  436\n",
      "==> Calculate AP for image_id :  437\n",
      "==> Calculate AP for image_id :  438\n",
      "==> Calculate AP for image_id :  439\n",
      "==> Calculate AP for image_id :  440\n",
      "==> Calculate AP for image_id :  441\n",
      "==> Calculate AP for image_id :  442\n",
      "==> Calculate AP for image_id :  443\n",
      "==> Calculate AP for image_id :  444\n",
      "==> Calculate AP for image_id :  445\n",
      "==> Calculate AP for image_id :  446\n",
      "==> Calculate AP for image_id :  447\n",
      "==> Calculate AP for image_id :  448\n",
      "==> Calculate AP for image_id :  449\n",
      "==> Calculate AP for image_id :  450\n",
      "==> Calculate AP for image_id :  451\n",
      "==> Calculate AP for image_id :  452\n",
      "==> Calculate AP for image_id :  453\n",
      "==> Calculate AP for image_id :  454\n",
      "==> Calculate AP for image_id :  455\n",
      "==> Calculate AP for image_id :  456\n",
      "==> Calculate AP for image_id :  457\n",
      "==> Calculate AP for image_id :  458\n",
      "==> Calculate AP for image_id :  459\n",
      "==> Calculate AP for image_id :  460\n",
      "==> Calculate AP for image_id :  461\n",
      "==> Calculate AP for image_id :  462\n",
      "==> Calculate AP for image_id :  463\n",
      "==> Calculate AP for image_id :  464\n",
      "==> Calculate AP for image_id :  465\n",
      "==> Calculate AP for image_id :  466\n",
      "==> Calculate AP for image_id :  467\n",
      "==> Calculate AP for image_id :  468\n",
      "==> Calculate AP for image_id :  469\n",
      "==> Calculate AP for image_id :  470\n",
      "==> Calculate AP for image_id :  471\n",
      "==> Calculate AP for image_id :  472\n",
      "==> Calculate AP for image_id :  473\n",
      "==> Calculate AP for image_id :  474\n",
      "==> Calculate AP for image_id :  475\n",
      "==> Calculate AP for image_id :  476\n",
      "==> Calculate AP for image_id :  477\n",
      "==> Calculate AP for image_id :  478\n",
      "==> Calculate AP for image_id :  479\n",
      "==> Calculate AP for image_id :  480\n",
      "==> Calculate AP for image_id :  481\n",
      "==> Calculate AP for image_id :  482\n",
      "==> Calculate AP for image_id :  483\n",
      "==> Calculate AP for image_id :  484\n",
      "==> Calculate AP for image_id :  485\n",
      "==> Calculate AP for image_id :  486\n",
      "==> Calculate AP for image_id :  487\n",
      "==> Calculate AP for image_id :  488\n",
      "==> Calculate AP for image_id :  489\n",
      "==> Calculate AP for image_id :  490\n",
      "==> Calculate AP for image_id :  491\n",
      "==> Calculate AP for image_id :  492\n",
      "==> Calculate AP for image_id :  493\n",
      "==> Calculate AP for image_id :  494\n",
      "==> Calculate AP for image_id :  495\n",
      "==> Calculate AP for image_id :  496\n",
      "==> Calculate AP for image_id :  497\n",
      "==> Calculate AP for image_id :  498\n",
      "==> Calculate AP for image_id :  499\n",
      "complete\n",
      "AP Calcs complete for epoch: 0150  Weight file: F:/models_newshapes/train_fcn8_l2_newshapes/fcn20181224T0000\\fcn_0150.h5\n",
      "1\n",
      "F:/models_newshapes/train_fcn8_l2_newshapes/fcn20181224T0000\\fcn_0150.h5 0150\n",
      " ***  Saved AP_results for epoch: 0150  Weight file: F:/models_newshapes/train_fcn8_l2_newshapes/fcn20181224T0000\\fcn_0150.h5\n",
      "      to ---->  E:\\git_projs\\MRCNN3\\train_newshapes\\eval_method2_results     Filename:  eval2_AP_results_2019_01_24\n",
      " ***  Save to : eval2_cls_info_epoch0150_500  --  eval2_pr_bboxes_epoch0150_500  --  eval2_gt_bboxes_epoch0150_500\n",
      " ***  Saves complete\n"
     ]
    }
   ],
   "source": [
    "for FILE_IDX in [1]:\n",
    "    weights_file = os.path.join(DIR_WEIGHTS  , files[FILE_IDX])\n",
    "    print(\"Loading weights \", weights_file)\n",
    "    fcn_model.load_model_weights(weights_file)\n",
    "\n",
    "    ###  Initialize data structures \n",
    "    orig_score = 5\n",
    "    norm_score = 8\n",
    "    alt_scr_0  = 11\n",
    "    alt_scr_1  = 14   # in MRCNN alt_scr_1 ans alt_scr_2 are the same\n",
    "    alt_scr_2  = 20\n",
    "    IMGS = 500\n",
    "    # shuffled_image_ids = np.copy(dataset_test.image_ids)\n",
    "    # np.random.shuffle(shuffled_image_ids)\n",
    "    # image_ids = np.random.choice(dataset_test.image_ids, 300)\n",
    "    image_ids = dataset_test.image_ids[:IMGS]\n",
    "    print(len(image_ids))\n",
    "\n",
    "    class_dict = []\n",
    "    gt_dict = {}\n",
    "    pr_dict = {}\n",
    "\n",
    "    for a,b in zip(dataset_test.class_ids, dataset_test.class_names):\n",
    "        class_dict.append({'id'   : int(a),\n",
    "                             'name' : b,\n",
    "                             'scores': [],\n",
    "                             'bboxes': [],\n",
    "                             'mrcnn_score_orig' : [],\n",
    "                             'mrcnn_score_norm' : [], \n",
    "                             'mrcnn_score_0' : [],\n",
    "                             'mrcnn_score_1' : [],\n",
    "                             'mrcnn_score_2' : [],\n",
    "                             'fcn_score_0' : [],\n",
    "                             'fcn_score_1' : [],\n",
    "                             'fcn_score_2' : [],                      \n",
    "                          })\n",
    "\n",
    "    # Compute VOC-Style mAP @ IoU=0.5\n",
    "    # Running on 10 images. Increase for better accuracy.\n",
    "\n",
    "\n",
    "    MRCNN_AP_Orig = []\n",
    "    MRCNN_AP_0 = [] \n",
    "    MRCNN_AP_1 = [] \n",
    "    MRCNN_AP_2 = []\n",
    "    FCN_AP_0   = []\n",
    "    FCN_AP_1   = []\n",
    "    FCN_AP_2   = []\n",
    "\n",
    "    for image_id in image_ids:\n",
    "        # Load image and ground truth data\n",
    "        print('==> Calculate AP for image_id : ', image_id)\n",
    "        # Run object detection\n",
    "        eval_batch = get_evaluate_batch(dataset_test, mrcnn_model.config, image_id, display = False)    \n",
    "        fcn_results = fcn_model.evaluate(mrcnn_model, eval_batch, verbose =0)    \n",
    "\n",
    "        gt_dict, pr_dict, class_dict = update_map_dictionaries(fcn_results, gt_dict,pr_dict, class_dict)\n",
    "\n",
    "        r = fcn_results[0] \n",
    "\n",
    "        #   Compute  AP, precisions, recalls, overlaps\n",
    "        AP, _, _, _= utils.compute_ap(r['gt_bboxes'], r['gt_class_ids'], r[\"molded_rois\"], r[\"class_ids\"], r[\"pr_scores\"][:,orig_score])\n",
    "        MRCNN_AP_Orig.append(AP)\n",
    "\n",
    "        AP, _, _, _= utils.compute_ap(r['gt_bboxes'], r['gt_class_ids'], r[\"molded_rois\"], r[\"class_ids\"], r[\"pr_scores\"][:,alt_scr_0])\n",
    "        MRCNN_AP_0.append(AP)\n",
    "\n",
    "        AP, _, _, _= utils.compute_ap(r['gt_bboxes'], r['gt_class_ids'], r[\"molded_rois\"], r[\"class_ids\"], r[\"fcn_scores\"][:,alt_scr_0])\n",
    "        FCN_AP_0.append(AP)\n",
    "\n",
    "        AP, _, _, _= utils.compute_ap(r['gt_bboxes'], r['gt_class_ids'], r[\"molded_rois\"], r[\"class_ids\"], r[\"pr_scores\"][:,alt_scr_1])\n",
    "        MRCNN_AP_1.append(AP)\n",
    "\n",
    "        AP, _, _, _= utils.compute_ap(r['gt_bboxes'], r['gt_class_ids'], r[\"molded_rois\"], r[\"class_ids\"], r[\"fcn_scores\"][:,alt_scr_1])\n",
    "        FCN_AP_1.append(AP)\n",
    "\n",
    "        AP, _, _, _= utils.compute_ap(r['gt_bboxes'], r['gt_class_ids'], r[\"molded_rois\"], r[\"class_ids\"], r[\"pr_scores\"][:,alt_scr_2])\n",
    "        MRCNN_AP_2.append(AP)\n",
    "\n",
    "        AP, _, _, _= utils.compute_ap(r['gt_bboxes'], r['gt_class_ids'], r[\"molded_rois\"], r[\"class_ids\"], r[\"fcn_scores\"][:,alt_scr_2])\n",
    "        FCN_AP_2.append(AP)\n",
    "\n",
    "    print('complete')\n",
    "\n",
    "    ### Append data to All_APResults\n",
    "    ###--------------------------------------------------------------------------\n",
    "    epochs = files[FILE_IDX].split('_')[1].replace('.h5','')\n",
    "    print('AP Calcs complete for epoch:', epochs , ' Weight file:', weights_file)\n",
    "\n",
    "    APResult = {}\n",
    "    APResult['Filename']      =  weights_file  \n",
    "    APResult['Epochs']        =  epochs\n",
    "    APResult['MRCNN_AP_Orig'] =  MRCNN_AP_Orig\n",
    "    APResult['MRCNN_AP_0'   ] =  MRCNN_AP_0   \n",
    "    APResult['MRCNN_AP_1'   ] =  MRCNN_AP_1   \n",
    "    APResult['MRCNN_AP_2'   ] =  MRCNN_AP_2   \n",
    "    APResult['FCN_AP_0'     ] =  FCN_AP_0     \n",
    "    APResult['FCN_AP_1'     ] =  FCN_AP_1     \n",
    "    APResult['FCN_AP_2'     ] =  FCN_AP_2     \n",
    "    All_APResults[weights_file] = APResult\n",
    "\n",
    "\n",
    "    # print('/'.join(weights_file.split('/')[-3:]))\n",
    "    print(len(All_APResults.keys()))\n",
    "    for i in sorted(All_APResults):\n",
    "        print(i, All_APResults[i]['Epochs'])\n",
    "\n",
    "    #### Save AP_Results\n",
    "    ####------------------------------------------------------------------------\n",
    "    with open(os.path.join(save_path, new_AP_results_file+'.pkl'), 'wb') as outfile:\n",
    "        pickle.dump(All_APResults, outfile)\n",
    "    print(' ***  Saved AP_results for epoch:',  All_APResults[i]['Epochs'], ' Weight file:', i)\n",
    "    print('      to ----> ', save_path,'    Filename: ', new_AP_results_file)\n",
    "\n",
    "    #### Save Cls_info, pr_bboxes dictionaries\n",
    "    ####------------------------------------------------------------------------\n",
    "    cls_info_file = file_prefix+'_cls_info_epoch'+epochs+'_'+str(len(image_ids))\n",
    "    pr_boxes_file = file_prefix+'_pr_bboxes_epoch'+epochs+'_'+str(len(image_ids))\n",
    "    gt_boxes_file = file_prefix+'_gt_bboxes_epoch'+epochs+'_'+str(len(image_ids))\n",
    "    print(' ***  Save to :', cls_info_file,' -- ', pr_boxes_file,' -- ', gt_boxes_file)\n",
    "\n",
    "    with open(os.path.join(save_path, cls_info_file+'.pkl'), 'wb') as outfile:\n",
    "        pickle.dump(class_dict, outfile)\n",
    "    with open(os.path.join(save_path, gt_boxes_file+'.pkl'), 'wb') as outfile:\n",
    "        pickle.dump(gt_dict, outfile)\n",
    "    with open(os.path.join(save_path, pr_boxes_file+'.pkl'), 'wb') as outfile:\n",
    "        pickle.dump(pr_dict, outfile)    \n",
    "    print(' ***  Saves complete')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-15T19:30:00.556115Z",
     "start_time": "2019-01-15T19:30:00.096788Z"
    },
    "hidden": true
   },
   "source": [
    "#### Display most recent AP_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-24T16:00:51.476947Z",
     "start_time": "2019-01-24T16:00:51.092010Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After 0150 training epochs.\n",
      "Weight file: F:/models_newshapes/train_fcn8_l2_newshapes/fcn20181224T0000\\fcn_0150.h5\n",
      "\n",
      "Images   Epochs   MRCNN_AP_Orig    MRCNN_AP_0      FCN_AP_0    MRCNN_AP_1      FCN_AP_1    MRCNN_AP_2      FCN_AP_2\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "10        0150          0.72884       0.67530       0.62764       0.65413       0.65807       0.65413       0.66184\n",
      "50        0150          0.65849       0.64609       0.61646       0.63625       0.64988       0.63449       0.65312\n",
      "100       0150          0.64663       0.63597       0.62774       0.62303       0.64708       0.62193       0.64752\n",
      "250       0150          0.65590       0.63106       0.64264       0.61944       0.64622       0.61820       0.64878\n",
      "500       0150          0.65855       0.63392       0.64722       0.62614       0.64937       0.62509       0.65132\n"
     ]
    }
   ],
   "source": [
    "####------------------------------------------------------------------------\n",
    "np_format = {}\n",
    "np_format['float'] = lambda x: \"%10.4f\" % x\n",
    "np_format['int']   = lambda x: \"%10d\" % x\n",
    "np.set_printoptions(linewidth=195, precision=4, floatmode='fixed', threshold =10000, formatter = np_format)\n",
    "\n",
    "# LIMIT = 10\n",
    "# print(\" {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s}\".format(\"MRCNN_AP_Orig\", \"MRCNN_AP_0\", \"FCN_AP_0\", \"MRCNN_AP_1\", \"FCN_AP_1\", \"MRCNN_AP_2\", \"FCN_AP_2\"))\n",
    "# for i in range(LIMIT):\n",
    "#     print(\" {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f}\".format(\n",
    "#         MRCNN_AP_Orig[i], MRCNN_AP_0[i], FCN_AP_0[i], MRCNN_AP_1[i], FCN_AP_1[i], MRCNN_AP_2[i], FCN_AP_2[i]))\n",
    "# print()\n",
    "# epochs = files[FILE_IDX].split('_')[1].replace('.h5','')\n",
    "print()\n",
    "print('After {} training epochs.\\nWeight file: {}'.format(epochs, weights_file))\n",
    "print()\n",
    "print(\"{:6s} {:^10s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s}\".format(\"Images\", \"Epochs\", \"MRCNN_AP_Orig\", \"MRCNN_AP_0\", \"FCN_AP_0\", \"MRCNN_AP_1\", \"FCN_AP_1\", \"MRCNN_AP_2\", \"FCN_AP_2\"))\n",
    "print('-'*116)\n",
    "\n",
    "for LIMIT in [10,50,100,250,500]:\n",
    "    print(\"{:<6d} {:^10s} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f}\".format(LIMIT, epochs,\n",
    "            np.mean(MRCNN_AP_Orig[:LIMIT]), \n",
    "            np.mean(MRCNN_AP_0[:LIMIT]), np.mean(FCN_AP_0[:LIMIT]), \n",
    "            np.mean(MRCNN_AP_1[:LIMIT]), np.mean(FCN_AP_1[:LIMIT]), \n",
    "            np.mean(MRCNN_AP_2[:LIMIT]), np.mean(FCN_AP_2[:LIMIT]) ))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save / Load AP Results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T15:32:49.414529Z",
     "start_time": "2019-01-17T15:32:48.685013Z"
    }
   },
   "outputs": [],
   "source": [
    "# AP_results_file= 'test_AP_results_2019_01_17'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save AP_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T10:39:00.541372Z",
     "start_time": "2019-01-17T10:38:59.796843Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(os.path.join(save_path, AP_results_file+'.pkl'), 'wb') as outfile:\n",
    "#     pickle.dump(All_APResults, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load AP Results from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:40:14.315641Z",
     "start_time": "2019-02-03T14:40:13.953382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: E:\\git_projs\\MRCNN3\\train_newshapes\\eval_method1_results_BCE2     Filename:  eval1_AP_results_2019_02_03\n"
     ]
    }
   ],
   "source": [
    "eval_method = '1'\n",
    "save_path = \"E:\\\\git_projs\\\\MRCNN3\\\\train_newshapes\\\\eval_method\"+eval_method+\"_results_BCE2\"\n",
    "file_prefix = 'eval'+eval_method\n",
    "new_AP_results_file = file_prefix+'_AP_results_2019_02_03'\n",
    "print('Path:' ,save_path, '    Filename: ', new_AP_results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:40:16.342084Z",
     "start_time": "2019-02-03T14:40:16.003843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "F:/models_newshapes/train_fcn8L2_BCE2/fcn20190131T0000\\fcn_0500.h5 0500\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del APRes \n",
    "except:\n",
    "    pass\n",
    "\n",
    "with open(os.path.join(save_path, new_AP_results_file+'.pkl'), 'rb') as outfile:\n",
    "    APRes = pickle.load(outfile)\n",
    "\n",
    "print(len(APRes.keys()))\n",
    "# pp.pprint(APRes.keys())\n",
    "for i in sorted(APRes):\n",
    "    print(i, APRes[i]['Epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display AP_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T14:40:18.784825Z",
     "start_time": "2019-02-03T14:40:18.418563Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "After 0500 training epochs.\n",
      "Weight file: F:/models_newshapes/train_fcn8L2_BCE2/fcn20190131T0000\\fcn_0500.h5\n",
      "\n",
      "Images   Epochs   MRCNN_AP_Orig    MRCNN_AP_0      FCN_AP_0    MRCNN_AP_1      FCN_AP_1    MRCNN_AP_2      FCN_AP_2\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "10        0500          0.67840       0.63939       0.63767       0.66053       0.62175       0.66053       0.62175\n",
      "50        0500          0.61057       0.61874       0.63106       0.62708       0.63497       0.62944       0.63497\n",
      "100       0500          0.62229       0.62584       0.64592       0.64531       0.64353       0.64403       0.64353\n",
      "250       0500          0.63571       0.63117       0.64394       0.63773       0.65321       0.63685       0.65321\n",
      "500       0500          0.63251       0.63391       0.63642       0.63615       0.64570       0.63558       0.64570\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(APRes) :\n",
    "#     print(' AP Result Entry :', key)\n",
    "    print()    \n",
    "    print()\n",
    "    print('After {} training epochs.\\nWeight file: {}'.format( APRes[key]['Epochs'],APRes[key]['Filename']))\n",
    "    print()\n",
    "    print(\"{:6s} {:^10s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s}\".format(\"Images\", \"Epochs\", \"MRCNN_AP_Orig\", \"MRCNN_AP_0\", \"FCN_AP_0\", \"MRCNN_AP_1\", \"FCN_AP_1\", \"MRCNN_AP_2\", \"FCN_AP_2\"))\n",
    "    print('-'*116)\n",
    "    for LIMIT in [10,50,100,250,500]:\n",
    "#     for LIMIT in [10]:\n",
    "        print(\"{:<6d} {:^10s} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f}\".format(LIMIT, APRes[key]['Epochs'],\n",
    "                np.mean(APRes[key]['MRCNN_AP_Orig'][:LIMIT]), \n",
    "                np.mean(APRes[key]['MRCNN_AP_0'][:LIMIT]), \n",
    "                np.mean(APRes[key]['FCN_AP_0'][:LIMIT]), \n",
    "                np.mean(APRes[key]['MRCNN_AP_1'][:LIMIT]),\n",
    "                np.mean(APRes[key]['FCN_AP_1'][:LIMIT]), \n",
    "                np.mean(APRes[key]['MRCNN_AP_2'][:LIMIT]), \n",
    "                np.mean(APRes[key]['FCN_AP_2'][:LIMIT]) ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-16T10:40:07.857700Z",
     "start_time": "2019-01-16T10:40:06.861619Z"
    }
   },
   "outputs": [],
   "source": [
    "# for key in sorted(APRes) :\n",
    "#     # print(' AP Result Entry :', key)\n",
    "#     print()\n",
    "#     print('After {} training epochs.\\nWeight file: {}'.format( APRes[key]['Epochs'],APRes[key]['Filename']))\n",
    "#     print()\n",
    "#     print(\"{:6s} {:^10s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s}\".format(\"Images\", \"Epochs\", \"MRCNN_AP_Orig\", \"MRCNN_AP_0\", \"FCN_AP_0\", \"MRCNN_AP_1\", \"FCN_AP_1\", \"MRCNN_AP_2\", \"FCN_AP_2\"))\n",
    "#     print('-'*116)\n",
    "#     for LIMIT in [10,50,100,250,500]:\n",
    "#         print(\"{:<6d} {:^10s} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f}\".format(LIMIT, APRes[key]['Epochs'],\n",
    "#                 np.mean(APRes[key]['MRCNN_AP_Orig'][:LIMIT]), \n",
    "#                 np.mean(APRes[key]['MRCNN_AP_0'][:LIMIT]), \n",
    "#                 np.mean(APRes[key]['FCN_AP_0'][:LIMIT]), \n",
    "#                 np.mean(APRes[key]['MRCNN_AP_1'][:LIMIT]),\n",
    "#                 np.mean(APRes[key]['FCN_AP_1'][:LIMIT]), \n",
    "#                 np.mean(APRes[key]['MRCNN_AP_2'][:LIMIT]), \n",
    "#                 np.mean(APRes[key]['FCN_AP_2'][:LIMIT]) ))\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-08T13:13:08.230108Z",
     "start_time": "2019-01-08T13:13:08.223121Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(\" {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s}\".format(\"MRCNN_AP_Orig\", \"MRCNN_AP_0\", \"FCN_AP_0\", \"MRCNN_AP_1\", \"FCN_AP_1\", \"MRCNN_AP_2\", \"FCN_AP_2\"))\n",
    "# for i in range(len(MRCNN_AP_Orig)):\n",
    "#     print(\" {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f}\".format(\n",
    "#         MRCNN_AP_Orig[i], MRCNN_AP_0[i], FCN_AP_0[i], MRCNN_AP_1[i], FCN_AP_1[i], MRCNN_AP_2[i], FCN_AP_2[i]))\n",
    "# print()\n",
    "\n",
    "# print()\n",
    "# print('Weight file used: {}   {}'.format(files[FILE_IDX], files[FILE_IDX].split('_')[1].rstrip('.h5')))\n",
    "# print()\n",
    "# print(\"{:<10s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s} {:>13s}\".format(\"Epochs\", \"MRCNN_AP_Orig\", \"MRCNN_AP_0\", \"FCN_AP_0\", \"MRCNN_AP_1\", \"FCN_AP_1\", \"MRCNN_AP_2\", \"FCN_AP_2\"))\n",
    "# print('-'*108)\n",
    "# print(\"{:<3d}-{:<7s} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f} {:13.5f}\".format(len(image_ids),files[FILE_IDX].split('_')[1].rstrip('.h5'),\n",
    "#         np.mean(MRCNN_AP_Orig), np.mean(MRCNN_AP_0), np.mean(FCN_AP_0), np.mean(MRCNN_AP_1), np.mean(FCN_AP_1), np.mean(MRCNN_AP_2), np.mean(FCN_AP_2)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T11:52:38.337361Z",
     "start_time": "2019-01-05T11:52:37.128449Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(r['image_meta'][:9])\n",
    "# print(r['gt_class_ids'])\n",
    "# print(r['gt_bboxes'])\n",
    "# print(r['class_ids']) \n",
    "# print(r['molded_rois'])\n",
    "# pr_scores_by_image = utils.byclass_to_byimage_np(r['pr_hm_scores'], 7)\n",
    "# print(pr_scores_by_image[:,:5])\n",
    "# print(r['pr_scores'].shape)\n",
    "# print(r['pr_scores'][:,:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T13:29:32.003642Z",
     "start_time": "2019-01-17T13:29:31.140028Z"
    }
   },
   "outputs": [],
   "source": [
    "cls_info_file = 'test_cls_info_epoch'+epochs+'_'+str(len(image_ids))\n",
    "pr_boxes_file = 'test_pr_bboxes_epoch'+epochs+'_'+str(len(image_ids))\n",
    "gt_boxes_file = 'test_gt_bboxes_epoch'+epochs+'_'+str(len(image_ids))\n",
    "print(cls_info_file,'   ', pr_boxes_file,'   ', gt_boxes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-17T13:29:37.553587Z",
     "start_time": "2019-01-17T13:29:36.447801Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(save_path, cls_info_file+'.pkl'), 'wb') as outfile:\n",
    "    pickle.dump(class_dict, outfile)\n",
    "with open(os.path.join(save_path, gt_boxes_file+'.pkl'), 'wb') as outfile:\n",
    "    pickle.dump(gt_dict, outfile)\n",
    "with open(os.path.join(save_path, pr_boxes_file+'.pkl'), 'wb') as outfile:\n",
    "    pickle.dump(pr_dict, outfile)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T12:34:44.191472Z",
     "start_time": "2019-01-10T12:34:43.446910Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open(os.path.join(save_path, cls_info_file+'.json'), 'w') as outfile:\n",
    "#     json.dump(class_dict, outfile)\n",
    "# with open(os.path.join(save_path, gt_boxes_file+'.json'), 'w') as outfile:\n",
    "#     json.dump(gt_dict, outfile)\n",
    "# with open(os.path.join(save_path, pr_boxes_file+'.json'), 'w') as outfile:\n",
    "#     json.dump(pr_dict, outfile)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:TF]",
   "language": "python",
   "name": "conda-env-TF-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
